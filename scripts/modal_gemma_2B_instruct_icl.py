"""
Modal script for evaluating ICL method with Instruct-style template on test set
- Loads dataset from volume
- Uses local model loading (not Inference API)
- Saves results to volume
- Supports GPU (A10) or CPU
- é»˜è®¤åœ¨æµ‹è¯•é›†ä¸Šè¿›è¡Œå…¨é‡è¯„ä¼°
- ä½¿ç”¨æŒ‡ä»¤å¼æ¨¡æ¿ï¼ˆInstruct templateï¼‰
- é»˜è®¤ä½¿ç”¨ Phi-2 (2.7B) æ¨¡å‹ï¼ˆå¼€æ”¾è®¿é—®ï¼‰

Usage:
  # ä½¿ç”¨é»˜è®¤æ¨¡å‹ (Phi-2 2.7B) - è¯„ä¼°æ•´ä¸ªæµ‹è¯•é›†
  modal run scripts/modal_gemma_2B_instruct_icl.py::evaluate
  
  # ä½¿ç”¨å…¶ä»–æ¨¡å‹ - è¯„ä¼°æ•´ä¸ªæµ‹è¯•é›†
  modal run scripts/modal_gemma_2B_instruct_icl.py::evaluate --model "stabilityai/stablelm-2-1_6b-instruct"
  
  # è¯„ä¼°æŒ‡å®šæ•°é‡çš„æµ‹è¯•é›†æ ·æœ¬
  modal run scripts/modal_gemma_2B_instruct_icl.py::evaluate --max-samples 50
  modal run scripts/modal_gemma_2B_instruct_icl.py::evaluate --max-samples 200
  
  å¯ç”¨çš„ 2B çº§åˆ«æ¨¡å‹ï¼ˆå¼€æ”¾è®¿é—®ï¼Œæ— éœ€ gatedï¼‰:
  - microsoft/phi-2 (æ¨èï¼Œ2.7B å‚æ•°ï¼Œå¼€æ”¾è®¿é—®)
  - stabilityai/stablelm-2-1_6b-instruct (1.6B å‚æ•°ï¼Œå¼€æ”¾è®¿é—®)
"""

import os
import json
import time
import torch
from pathlib import Path
from datetime import datetime
from typing import Dict, List, Optional
from transformers import AutoTokenizer, AutoModelForCausalLM
from datasets import load_from_disk
import modal
from modal import Volume, Secret


# ICL Template - Instruct Style
ICL_TEMPLATE = """You are a clinical assistant specializing in structured medical reasoning.

Given a patient's question or description, produce a MECE-style hierarchical analysis.

Requirements:
1. Use numbered sections and subsections (1, 1.1, 1.2 â€¦).
2. Each section must follow MECE (mutually exclusive, collectively exhaustive).
3. No diagnosis or prescriptions; list only plausible causes + safe, general advice.
4. Keep bullets concise, factual, and non-overlapping.
5. Output **only** the structured hierarchy. No explanations or extra text.

=== FEW-SHOT EXAMPLES ===

Q: hi doctor my mom aged 45 years is having a fever for almost two months the temperature used to vary from 100 degrees celsius in the night times and gradually it comes down she was weighing 105 keg and now she is keg now she is having bone pain and light fever in the night when there is a pain in the bone she has been advised with calcium and iron tablets she is also getting her regular periods menopause has not reached yet kindly advise
A:
- **1. Clinical Presentation**
  - **1.1 Fever Pattern**
    - Low-grade nocturnal fever
    - Duration around two months
  - **1.2 Systemic Features**
    - Bone pain
    - Notable weight loss

- **2. Plausible Underlying Causes**
  - **2.1 Infectious Etiologies**
    - Chronic infections such as tuberculosis
  - **2.2 Non-Infectious Etiologies**
    - Metabolic, inflammatory, or autoimmune processes

- **3. Recommended Evaluations**
  - **3.1 Baseline Tests**
    - CBC, ESR/CRP
  - **3.2 TB-Specific Tests**
    - Mantoux or IGRA screening

- **4. Supportive Measures**
  - **4.1 Hydration and Diet**
    - Maintain fluid intake
    - Ensure adequate nutrition

- **5. Follow-up Plan**
  - **5.1 Next Steps**
    - Complete evaluations
    - Reassess based on results


Q: hi doctor I am having a problem with sinusitis my doctor ordered me to take co altria 10 mg at bedtime I have taken it in the morning and currently I am experiencing dryness of mouth and headache what to do is this serious I am also taking cefixime 200 mg twice daily sinupret thrice daily and nasoflo spray
A:
- **1. Symptom Explanation**
  - **1.1 Medication Effects**
    - Anticholinergic action may reduce saliva
    - Headache can occur from timing mismatch
  - **1.2 Expected Duration**
    - Often transient
    - Usually improves as body adapts

- **2. Possible Contributing Factors**
  - **2.1 Morning Dose Error**
    - Taking sedating meds early may worsen side effects
  - **2.2 Combination Therapy**
    - Multiple agents can amplify dryness

- **3. Safe Management**
  - **3.1 Immediate Steps**
    - Maintain hydration
    - Monitor symptom progression
  - **3.2 When to Seek Help**
    - Persistent headache
    - Worsening dryness impacting eating or speech


Q: hello doctor I have erosion in my stomach severe burning sensation and sometimes vomiting and bloating I have done endoscopy colonoscopy angiography and hospitalized for about four times recently again I have done endoscopy found astral erosion please advice
A:
- **1. Symptom Profile**
  - **1.1 Upper GI Symptoms**
    - Burning epigastric pain
    - Bloating and nausea
  - **1.2 Course of Illness**
    - Recurrent episodes despite evaluation

- **2. Potential Contributing Factors**
  - **2.1 Gastric Irritation**
    - Acid-related mucosal injury
  - **2.2 Lifestyle Irritants**
    - Spicy food, irregular meals

- **3. Non-Prescriptive Management**
  - **3.1 Behavioral Measures**
    - Smaller frequent meals
    - Avoid irritants such as caffeine/spices
  - **3.2 Monitoring**
    - Track symptom triggers
    - Watch for red-flag symptoms (vomiting blood, weight loss)

- **4. Information Needed**
  - **4.1 Prior Reports**
    - Endoscopy findings trend
  - **4.2 Infection Status**
    - Whether H. pylori testing was done


=== NOW ANSWER IN THE SAME MECE STYLE ===

Q: {{question}}

A:
"""


def create_prompt(patient_description: str) -> str:
    """åˆ›å»º ICL promptï¼Œä½¿ç”¨æ‚£è€…æè¿°ï¼ˆPatientå­—æ®µï¼‰ä½œä¸ºè¾“å…¥"""
    # æ³¨æ„ï¼šICLæ¨¡æ¿ä¸­ä½¿ç”¨ {{question}} ä½œä¸ºå ä½ç¬¦ï¼Œä½†å®é™…ä¼ å…¥çš„æ˜¯ Patient å­—æ®µï¼ˆæ‚£è€…è¯¦ç»†æè¿°ï¼‰
    return ICL_TEMPLATE.replace("{{question}}", patient_description)


def call_local_model(
    model,
    tokenizer,
    device,
    prompt: str,
    max_new_tokens: int = 1024,
    temperature: float = 0.7,
    top_p: float = 0.9,
) -> Optional[str]:
    """
    ä½¿ç”¨æœ¬åœ°åŠ è½½çš„æ¨¡å‹ç”Ÿæˆå“åº”
    
    Args:
        model: æœ¬åœ°åŠ è½½çš„æ¨¡å‹
        tokenizer: åˆ†è¯å™¨
        device: è®¾å¤‡ï¼ˆcuda/cpuï¼‰
        prompt: è¾“å…¥æç¤º
        max_new_tokens: æœ€å¤§ç”Ÿæˆtokenæ•°
        temperature: é‡‡æ ·æ¸©åº¦
        top_p: Nucleus samplingå‚æ•°
        
    Returns:
        ç”Ÿæˆçš„å“åº”æ–‡æœ¬æˆ– Noneï¼ˆå¦‚æœå¤±è´¥ï¼‰
    """
    try:
        inputs = tokenizer(prompt, return_tensors="pt").to(device)
        input_length = inputs.input_ids.shape[1]  # è®°å½• prompt çš„é•¿åº¦
        
        model.eval()
        
        with torch.no_grad():
            outputs = model.generate(
                **inputs,
                max_new_tokens=max_new_tokens,
                min_new_tokens=10,  # ç¡®ä¿è‡³å°‘ç”Ÿæˆ10ä¸ªtoken
                temperature=temperature,
                top_p=top_p,
                do_sample=True,
                repetition_penalty=1.2,
                eos_token_id=tokenizer.eos_token_id,
                pad_token_id=tokenizer.pad_token_id if tokenizer.pad_token_id else tokenizer.eos_token_id,
                no_repeat_ngram_size=3,  # é¿å…é‡å¤çš„3-gram
            )
        
        # åªè§£ç ç”Ÿæˆçš„éƒ¨åˆ†ï¼ˆä» input_length å¼€å§‹ï¼‰
        generated_ids = outputs[0][input_length:]
        
        # è°ƒè¯•ï¼šæ£€æŸ¥ç”Ÿæˆçš„ token æ•°é‡
        if len(generated_ids) == 0:
            print(f"  âš  ç”Ÿæˆäº†0ä¸ªtokenï¼ˆoutput_length={len(outputs[0])}, input_length={input_length}ï¼‰")
            return None
        
        # è¿‡æ»¤æ‰ EOS å’Œ PAD token
        generated_ids_filtered = [
            token_id for token_id in generated_ids 
            if token_id not in [tokenizer.eos_token_id, tokenizer.pad_token_id]
        ]
        
        if len(generated_ids_filtered) == 0:
            print(f"  âš  ç”Ÿæˆçš„æ‰€æœ‰tokenéƒ½æ˜¯ç‰¹æ®Štokenï¼ˆEOS/PADï¼‰")
            return None
        
        generated_text = tokenizer.decode(generated_ids_filtered, skip_special_tokens=True).strip()
        
        # å¦‚æœç”Ÿæˆçš„å†…å®¹ä¸ºç©ºï¼Œè¿”å› None
        if not generated_text:
            # å°è¯•ä¸è§£ç ç”Ÿæˆçš„æ‰€æœ‰ token çœ‹çœ‹
            all_text = tokenizer.decode(generated_ids, skip_special_tokens=False)
            print(f"  âš  ç”Ÿæˆäº†ç©ºå†…å®¹ï¼ˆåŸå§‹è§£ç : {all_text[:50]}...ï¼‰")
            return None
        
        return generated_text
        
    except Exception as e:
        error_msg = str(e)
        error_type = type(e).__name__
        print(f"  âœ— æœ¬åœ°æ¨¡å‹ç”Ÿæˆå¤±è´¥")
        print(f"     é”™è¯¯ç±»å‹: {error_type}")
        print(f"     é”™è¯¯ä¿¡æ¯: {error_msg[:200]}...")  # åªæ˜¾ç¤ºå‰200ä¸ªå­—ç¬¦
        import traceback
        traceback.print_exc()
        return None


def load_local_model(model_name: str, device: str = "cuda", hf_token: Optional[str] = None):
    """
    åŠ è½½æœ¬åœ°æ¨¡å‹å’Œåˆ†è¯å™¨
    
    Args:
        model_name: æ¨¡å‹åç§°
        device: è®¾å¤‡ï¼ˆcuda/cpuï¼‰
        hf_token: Hugging Face tokenï¼ˆç”¨äºè®¿é—® gated æ¨¡å‹ï¼‰
        
    Returns:
        (model, tokenizer, device) å…ƒç»„
    """
    print(f"  æ­£åœ¨åŠ è½½æ¨¡å‹ {model_name} åˆ°è®¾å¤‡ {device}...")
    
    # è·å– HF tokenï¼ˆç”¨äº gated æ¨¡å‹å¦‚ Llama/Gemmaï¼‰
    if hf_token is None:
        hf_token = os.environ.get("HF_TOKEN") or os.environ.get("HUGGING_FACE_HUB_TOKEN")
    
    # å‡†å¤‡ tokenizer å‚æ•°
    tokenizer_kwargs = {}
    if hf_token:
        tokenizer_kwargs["token"] = hf_token
        print(f"  âœ“ ä½¿ç”¨ Hugging Face token è®¿é—®æ¨¡å‹")
    
    # Gemma æ¨¡å‹å¯èƒ½éœ€è¦ tokenï¼Œå°è¯•åŠ è½½
    try:
        tokenizer = AutoTokenizer.from_pretrained(
            model_name, 
            trust_remote_code=True,
            **tokenizer_kwargs
        )
    except Exception as e1:
        # å¦‚æœå¤±è´¥ï¼Œå°è¯•ä¸ä½¿ç”¨ trust_remote_code
        try:
            tokenizer = AutoTokenizer.from_pretrained(
                model_name,
                **tokenizer_kwargs
            )
        except Exception as e2:
            print(f"  âœ— Tokenizer åŠ è½½å¤±è´¥")
            print(f"     é”™è¯¯1: {str(e1)[:200]}")
            print(f"     é”™è¯¯2: {str(e2)[:200]}")
            raise
    
    # è®¾ç½® pad tokenï¼ˆGemma æ¨¡å‹é€šå¸¸éœ€è¦è¿™ä¸ªè®¾ç½®ï¼‰
    if tokenizer.pad_token is None:
        tokenizer.pad_token = tokenizer.eos_token
    
    # ç¡®å®š dtype
    if device == "cuda":
        dtype = torch.float16
    else:
        dtype = torch.float32
    
    # å‡†å¤‡ model å‚æ•°
    model_kwargs = {
        "dtype": dtype,
        "device_map": "auto" if device == "cuda" else None,
    }
    if hf_token:
        model_kwargs["token"] = hf_token
    
    # Gemma æ¨¡å‹åŠ è½½
    try:
        model = AutoModelForCausalLM.from_pretrained(
            model_name,
            trust_remote_code=True,
            **model_kwargs
        )
    except Exception:
        # å¦‚æœå¤±è´¥ï¼Œå°è¯•ä¸ä½¿ç”¨ trust_remote_code
        model = AutoModelForCausalLM.from_pretrained(
            model_name,
            **model_kwargs
        )
    
    if device == "cpu":
        model = model.to(device)
    
    print(f"  âœ“ æ¨¡å‹åŠ è½½æˆåŠŸ")
    
    return model, tokenizer, device


def print_memory_info(device):
    """Print GPU memory information"""
    if device.type == "cuda":
        print("\n" + "=" * 60)
        print("GPU Memory Information")
        print("=" * 60)
        allocated = torch.cuda.memory_allocated(device) / 1e9
        reserved = torch.cuda.memory_reserved(device) / 1e9
        total = torch.cuda.get_device_properties(device).total_memory / 1e9
        print(f"Allocated: {allocated:.2f}GB")
        print(f"Reserved: {reserved:.2f}GB")
        print(f"Total GPU: {total:.2f}GB")
        print(f"Available: {(total - reserved):.2f}GB")
        print("=" * 60 + "\n")


def evaluate_icl(
    model_name: str = "microsoft/phi-2",  # 2.7B å‚æ•°ï¼ŒPhi-2 æ¨¡å‹ï¼ˆå¼€æ”¾è®¿é—®ï¼Œæ— éœ€ gatedï¼‰
    max_new_tokens: int = 1024,
    temperature: float = 0.7,
    top_p: float = 0.9,
    max_samples: Optional[int] = None,  # é»˜è®¤è¯„ä¼°æ•´ä¸ªæµ‹è¯•é›†ï¼Œå¯æŒ‡å®šå…¶ä»–æ•°é‡
    hf_token: Optional[str] = None,  # Hugging Face tokenï¼ˆç”¨äºè®¿é—® gated æ¨¡å‹å¦‚ Gemmaï¼‰
    results_volume_obj=None,
):
    """
    åœ¨æµ‹è¯•é›†ä¸Šè¯„ä¼° ICL æ–¹æ³•ï¼ˆé»˜è®¤è¯„ä¼°å…¨éƒ¨æ ·æœ¬ï¼‰- ä½¿ç”¨æŒ‡ä»¤å¼æ¨¡æ¿
    
    Args:
        model_name: æ¨¡å‹åç§°ï¼ˆé»˜è®¤ Phi-2 2.7Bï¼‰
        max_new_tokens: æœ€å¤§ç”Ÿæˆtokenæ•°
        temperature: é‡‡æ ·æ¸©åº¦
        top_p: Nucleus samplingå‚æ•°
        max_samples: è¯„ä¼°æ ·æœ¬æ•°ï¼ˆé»˜è®¤Noneè¡¨ç¤ºå…¨éƒ¨æµ‹è¯•é›†æ ·æœ¬ï¼Œå¯æŒ‡å®šå…¶ä»–æ•°é‡ï¼‰
        results_volume_obj: Modal volumeå¯¹è±¡ï¼Œç”¨äºä¿å­˜ç»“æœ
        
    Returns:
        è¯„ä¼°ç»“æœæ‘˜è¦
    """
    # åœ¨ Modal GPU ä¸Šæ€»æ˜¯ä½¿ç”¨ CUDA
    # åœ¨ Modal ä¸Šï¼Œå¦‚æœé…ç½®äº† GPUï¼Œåˆ™ä½¿ç”¨ CUDA
    device_str = "cuda" if torch.cuda.is_available() else "cpu"
    device = torch.device(device_str)
    
    print("=" * 60)
    print("ICL æ–¹æ³•è¯„ä¼° - æµ‹è¯•é›† (Modal) - Phi-2")
    print("=" * 60)
    print(f"æ¨¡å‹: {model_name}")
    print(f"è®¾å¤‡: {device}")
    print(f"è¯„ä¼°æ ·æœ¬æ•°: {max_samples or 'å…¨éƒ¨'}")
    
    # åŠ è½½æ•°æ®é›†
    print("\nåŠ è½½æ•°æ®é›†...")
    dataset_path = "/dataset/hierarchical_dataset_clean"
    
    # æ£€æŸ¥æ•°æ®é›†æ˜¯å¦å­˜åœ¨
    import os
    if not os.path.exists(dataset_path):
        error_msg = f"""
âœ— é”™è¯¯: æ•°æ®é›†è·¯å¾„ä¸å­˜åœ¨: {dataset_path}

è¯·å…ˆä¸Šä¼ æ•°æ®é›†åˆ° Modal Volumeã€‚ä½ å¯ä»¥ï¼š

1. ä»æœ¬åœ°æ•°æ®é›†ä¸Šä¼ ï¼š
   modal volume put medical-dataset-volume \\
     ./data/processed/hierarchical_dataset_clean \\
     hierarchical_dataset_clean

2. æˆ–è€…æ£€æŸ¥æ•°æ®é›†æ˜¯å¦åœ¨ volume ä¸­çš„å…¶ä»–è·¯å¾„ï¼š
   modal volume ls medical-dataset-volume
"""
        print(error_msg)
        raise FileNotFoundError(f"Dataset not found at {dataset_path}. Please upload it first.")
    
    # ä»å®Œæ•´æ•°æ®é›†åŠ è½½
    print(f"ä» {dataset_path} åŠ è½½æ•°æ®é›†...")
    dataset = load_from_disk(dataset_path)
    print(f"âœ“ æ•°æ®é›†åŠ è½½æˆåŠŸï¼Œå…± {len(dataset)} ä¸ªæ ·æœ¬")
    
    # ä½¿ç”¨ç›¸åŒçš„åˆ’åˆ†æ–¹å¼
    print("åˆ’åˆ†è®­ç»ƒé›†å’Œæµ‹è¯•é›†ï¼ˆtest_size=0.1, seed=42ï¼‰...")
    split_dataset = dataset.train_test_split(test_size=0.1, seed=42)
    test_dataset = split_dataset["test"]
    
    print(f"âœ“ æµ‹è¯•é›†åŠ è½½æˆåŠŸï¼Œå…± {len(test_dataset)} ä¸ªæ ·æœ¬")
    
    # ç¡®å®šè¯„ä¼°æ ·æœ¬æ•°
    if max_samples is None:
        num_samples = len(test_dataset)
    else:
        num_samples = min(max_samples, len(test_dataset))
    print(f"\nå°†ä»æµ‹è¯•é›†ä¸­è¯„ä¼° {num_samples} ä¸ªæ ·æœ¬ï¼ˆæµ‹è¯•é›†å…± {len(test_dataset)} ä¸ªï¼‰")
    
    # åŠ è½½æ¨¡å‹
    print(f"\n{'='*60}")
    print("åŠ è½½æ¨¡å‹")
    print(f"{'='*60}")
    model, tokenizer, device_str = load_local_model(model_name, device_str, hf_token=hf_token)
    device = torch.device(device_str)
    
    print_memory_info(device)
    
    # åˆ›å»ºè¾“å‡ºç›®å½•
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    output_dir = f"/results/icl_eval_test_phi2_instruct_{timestamp}"
    os.makedirs(output_dir, exist_ok=True)
    
    results = []
    failed_count = 0
    
    # æ¯20ä¸ªæ ·æœ¬ä¿å­˜ä¸€æ¬¡ä¸­é—´ç»“æœ
    checkpoint_interval = 20
    
    print(f"\n{'='*60}")
    print("å¼€å§‹è¯„ä¼°")
    print(f"{'='*60}\n")
    
    for i in range(num_samples):
        sample = test_dataset[i]
        
        # æ„å»ºé—®é¢˜ï¼ˆä½¿ç”¨ Patient ä½œä¸ºè¾“å…¥ï¼ŒåŒ…å«æ‚£è€…çš„è¯¦ç»†æè¿°ï¼‰
        patient_input = sample.get('Patient', '')
        if not patient_input:
            # å¦‚æœ Patient ä¸ºç©ºï¼Œå›é€€åˆ° Description
            patient_input = sample.get('Description', '')
        
        print(f"\n[{i+1}/{num_samples}] å¤„ç†é—®é¢˜...")
        print(f"æ‚£è€…æè¿°: {patient_input[:100]}..." if len(patient_input) > 100 else f"æ‚£è€…æè¿°: {patient_input}")
        
        # ä½¿ç”¨ ICL ç”Ÿæˆç­”æ¡ˆï¼ˆä¼ å…¥æ‚£è€…æè¿°ï¼‰
        prompt = create_prompt(patient_input)
        
        start_time = time.time()
        generated_answer = call_local_model(
            model,
            tokenizer,
            device,
            prompt,
            max_new_tokens=max_new_tokens,
            temperature=temperature,
            top_p=top_p,
        )
        elapsed_time = time.time() - start_time
        
        if generated_answer:
            print(f"  âœ“ ç”ŸæˆæˆåŠŸ (è€—æ—¶: {elapsed_time:.2f}s)")
            print(f"  ç­”æ¡ˆé•¿åº¦: {len(generated_answer)} å­—ç¬¦")
        else:
            print(f"  âœ— ç”Ÿæˆå¤±è´¥")
            failed_count += 1
        
        # ä¿å­˜ç»“æœ
        result = {
            "index": i,
            "description": sample.get('Description', ''),  # ç®€çŸ­çš„é—®é¢˜æè¿°
            "patient_description": patient_input,  # æ‚£è€…è¯¦ç»†æè¿°ï¼ˆç”¨äºICLè¾“å…¥ï¼‰
            "reference_answer": sample.get('Doctor', ''),
            "generated_answer": generated_answer if generated_answer else "FAILED",
            "status": sample.get('Status', ''),
            "generation_time": elapsed_time,
            "success": generated_answer is not None,
        }
        results.append(result)
        
        # æ¯20ä¸ªæ ·æœ¬ä¿å­˜ä¸€æ¬¡ä¸­é—´ç»“æœ
        if (i + 1) % checkpoint_interval == 0:
            checkpoint_file = os.path.join(output_dir, f"icl_results_checkpoint_{i+1}.json")
            with open(checkpoint_file, 'w', encoding='utf-8') as f:
                json.dump(results, f, indent=2, ensure_ascii=False)
            print(f"  ğŸ’¾ å·²ä¿å­˜æ£€æŸ¥ç‚¹ ({i+1}/{num_samples}): {checkpoint_file}")
            
            # æäº¤ volume æ›´æ–°
            if results_volume_obj:
                results_volume_obj.commit()
    
    # ä¿å­˜æœ€ç»ˆç»“æœ
    results_file = os.path.join(output_dir, f"icl_results_{timestamp}.json")
    
    with open(results_file, 'w', encoding='utf-8') as f:
        json.dump(results, f, indent=2, ensure_ascii=False)
    
    print(f"\n{'='*60}")
    print("è¯„ä¼°å®Œæˆ")
    print(f"{'='*60}")
    print(f"æ€»æ ·æœ¬æ•°: {num_samples}")
    print(f"æˆåŠŸ: {num_samples - failed_count}")
    print(f"å¤±è´¥: {failed_count}")
    print(f"æˆåŠŸç‡: {(num_samples - failed_count) / num_samples * 100:.1f}%")
    print(f"\nç»“æœå·²ä¿å­˜åˆ°: {results_file}")
    
    # ç”Ÿæˆæ‘˜è¦
    summary = {
        "model": model_name,
        "template_type": "instruct",  # æ ‡è¯†è¿™æ˜¯æŒ‡ä»¤å¼æ¨¡æ¿
        "total_samples": num_samples,
        "successful": num_samples - failed_count,
        "failed": failed_count,
        "success_rate": (num_samples - failed_count) / num_samples * 100,
        "average_generation_time": sum(r['generation_time'] for r in results) / len(results) if results else 0,
        "results_file": results_file,
        "timestamp": timestamp,
        "device": device_str,
    }
    
    summary_file = os.path.join(output_dir, f"icl_summary_{timestamp}.json")
    with open(summary_file, 'w', encoding='utf-8') as f:
        json.dump(summary, f, indent=2, ensure_ascii=False)
    
    print(f"æ‘˜è¦å·²ä¿å­˜åˆ°: {summary_file}")
    
    # æäº¤ volume æ›´æ–°
    if results_volume_obj:
        results_volume_obj.commit()
    
    print_memory_info(device)
    
    return summary


# Create Modal app and volumes
app = modal.App("medical-icl-evaluation-gemma2b-instruct")

# Create volumes for dataset and results
dataset_volume = Volume.from_name("medical-dataset-volume", create_if_missing=True)
results_volume = Volume.from_name("medical-results-volume", create_if_missing=True)

# Docker image with necessary dependencies
image = modal.Image.debian_slim().pip_install(
    "torch",
    "transformers",
    "datasets",
    "accelerate",
)


@app.function(
    image=image,
    gpu="A10",  # å¯ä»¥æ”¹ä¸º None ä½¿ç”¨ CPUï¼Œæˆ– "A10G" ä½¿ç”¨æ›´ä¾¿å®œçš„ GPUï¼Œæˆ– "T4" ä½¿ç”¨æ›´ä¾¿å®œçš„ GPU
    volumes={
        "/dataset": dataset_volume,
        "/results": results_volume,
    },
    # secrets=[Secret.from_name("huggingface-secret")],  # Phi-2 ä¸éœ€è¦ gated accessï¼Œå¯ä»¥æ³¨é‡Šæ‰
    timeout=86400,  # 24å°æ—¶è¶…æ—¶
)
def evaluate(
    model: str = "microsoft/phi-2",  # 2.7B å‚æ•°ï¼ŒPhi-2 æ¨¡å‹ï¼ˆå¼€æ”¾è®¿é—®ï¼Œæ— éœ€ gatedï¼‰
    max_new_tokens: int = 1024,
    temperature: float = 0.7,
    top_p: float = 0.9,
    max_samples: Optional[int] = None,  # é»˜è®¤è¯„ä¼°æ•´ä¸ªæµ‹è¯•é›†ï¼Œå¯æŒ‡å®šå…¶ä»–æ•°é‡
):
    """
    Entrypoint for modal run command - é»˜è®¤è¯„ä¼°æ•´ä¸ªæµ‹è¯•é›†ï¼ˆä½¿ç”¨æŒ‡ä»¤å¼æ¨¡æ¿ + Phi-2ï¼‰
    
    Example:
        # è¯„ä¼°æ•´ä¸ªæµ‹è¯•é›† - é»˜è®¤è¡Œä¸ºï¼ˆä½¿ç”¨ Phi-2 2.7Bï¼‰
        modal run scripts/modal_gemma_2B_instruct_icl.py::evaluate
        
        # ä½¿ç”¨å…¶ä»–æ¨¡å‹ï¼š
        modal run scripts/modal_gemma_2B_instruct_icl.py::evaluate --model "stabilityai/stablelm-2-1_6b-instruct"
        
        å¯ç”¨çš„ 2B çº§åˆ«æ¨¡å‹é€‰é¡¹ï¼ˆå¼€æ”¾è®¿é—®ï¼‰ï¼š
        - microsoft/phi-2 (2.7B, æ¨èï¼Œæ— éœ€ gated access)
        - stabilityai/stablelm-2-1_6b-instruct (1.6B, æ— éœ€ gated access)
        
        # è¯„ä¼°æŒ‡å®šæ•°é‡çš„æµ‹è¯•é›†æ ·æœ¬
        modal run scripts/modal_gemma_2B_instruct_icl.py::evaluate --max-samples 50
        modal run scripts/modal_gemma_2B_instruct_icl.py::evaluate --max-samples 200
    """
    # Phi-2 ä¸éœ€è¦ gated accessï¼Œä½†å¦‚æœä½¿ç”¨å…¶ä»– gated æ¨¡å‹ï¼Œéœ€è¦ä»ç¯å¢ƒå˜é‡è·å– HF token
    hf_token = os.environ.get("HF_TOKEN") or os.environ.get("HUGGING_FACE_HUB_TOKEN")
    
    # åœ¨å‡½æ•°å†…éƒ¨è®¿é—® volume
    return evaluate_icl(
        model_name=model,
        max_new_tokens=max_new_tokens,
        temperature=temperature,
        top_p=top_p,
        max_samples=max_samples,
        hf_token=hf_token,
        results_volume_obj=results_volume,
    )


