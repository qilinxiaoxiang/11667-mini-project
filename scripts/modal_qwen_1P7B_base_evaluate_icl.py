"""
Modal script for evaluating ICL method on test set
- Loads test dataset from volume
- Uses local model loading (not Inference API)
- Saves results to volume
- Supports GPU (A100) or CPU

Usage:
  modal run scripts/modal_evaluate_icl.py::evaluate --model "Qwen/Qwen2.5-0.5B" --max-samples 10
  modal run scripts/modal_evaluate_icl.py::evaluate --model "Qwen/Qwen2.5-1.5B" --max-samples 50
"""

import os
import json
import time
import torch
from pathlib import Path
from datetime import datetime
from typing import Dict, List, Optional
from transformers import AutoTokenizer, AutoModelForCausalLM
from datasets import load_from_disk
import modal
from modal import Volume


# ICL Template
ICL_TEMPLATE = """Task: Convert a medical/nutrition question into a MECE hierarchical bullet answer.

You MUST follow the exact output constraints:

=== Output Format (STRICT) ===

- **1. <Top-level section name>**

  - **1.1 <Subsection name>**

    - <bullet>

    - <bullet>

- **2. <Top-level section name>**

  - **2.1 <Subsection name>**

    - <bullet>

...

Rules:

A) Use 5â€“7 top-level sections if applicable.

B) Subsections must be MECE within each top-level section.

C) No diagnosis; only plausible causes + safe advice.

D) No extra text before or after the hierarchy.

=== Fully-worked Example ===

Input:

"I eat very little, feel weak, and want to gain weight safely. What should I do?"

Output:

- **1. Daily calorie & eating pattern**

  - **1.1 Total intake**

    - Current intake is likely below needs; aim to increase gradually.

    - Avoid extremely low-calorie patterns.

  - **1.2 Meal structure**

    - 3 meals + 1 snack.

    - Add a light meal after protein-rich meals.

  - **1.3 Gradual adjustment**

    - Increase calories slowly over weeks.

    - Appetite may rise with consistent activity.

- **2. Recommended foods**

  - **2.1 High-calorie add-ons**

    - Ghee 2â€“3 tsp/day if tolerated.

    - Cheese/paneer ~3Ã— weekly.

  - **2.2 Protein sources**

    - Dals/whole pulses daily.

    - Protein supplement post-exercise if needed.

  - **2.3 Drinks & dairy**

    - Milkshakes or lassi for extra calories.

  - **2.4 Produce**

    - Include diverse fruits and vegetables daily.

- **3. Exercise guidance**

  - **3.1 Frequency**

    - Daily or near-daily.

  - **3.2 Duration**

    - 45â€“60 minutes.

  - **3.3 Type**

    - Brisk walking is sufficient to start.

- **4. Health expectations**

  - **4.1 Appetite**

    - Often improves with regular exercise.

  - **4.2 Weight**

    - Expect gradual gain, not rapid change.

  - **4.3 Immunity**

    - Adequate nutrition supports immunity, but avoid guarantees.

- **5. Psychological & lifestyle advice**

  - **5.1 Mental practices**

    - Meditation or stress reduction if anxiety affects eating.

  - **5.2 Mindset**

    - Focus on overall health, not only weight.

- **6. Follow-up**

  - **6.1 Timeframe**

    - Reassess after ~2 weeks.

=== Now do the same for: ===

Input:

{{medical_question}}

Output:"""


def create_prompt(medical_question: str) -> str:
    """åˆ›å»º ICL prompt"""
    return ICL_TEMPLATE.replace("{{medical_question}}", medical_question)


def call_local_model(
    model,
    tokenizer,
    device,
    prompt: str,
    max_new_tokens: int = 1024,
    temperature: float = 0.7,
    top_p: float = 0.9,
) -> Optional[str]:
    """
    ä½¿ç”¨æœ¬åœ°åŠ è½½çš„æ¨¡å‹ç”Ÿæˆå“åº”
    
    Args:
        model: æœ¬åœ°åŠ è½½çš„æ¨¡å‹
        tokenizer: åˆ†è¯å™¨
        device: è®¾å¤‡ï¼ˆcuda/cpuï¼‰
        prompt: è¾“å…¥æç¤º
        max_new_tokens: æœ€å¤§ç”Ÿæˆtokenæ•°
        temperature: é‡‡æ ·æ¸©åº¦
        top_p: Nucleus samplingå‚æ•°
        
    Returns:
        ç”Ÿæˆçš„å“åº”æ–‡æœ¬æˆ– Noneï¼ˆå¦‚æœå¤±è´¥ï¼‰
    """
    try:
        inputs = tokenizer(prompt, return_tensors="pt").to(device)
        input_length = inputs.input_ids.shape[1]  # è®°å½• prompt çš„é•¿åº¦
        
        model.eval()
        
        with torch.no_grad():
            outputs = model.generate(
                **inputs,
                max_new_tokens=max_new_tokens,
                min_new_tokens=10,  # ç¡®ä¿è‡³å°‘ç”Ÿæˆ10ä¸ªtoken
                temperature=temperature,
                top_p=top_p,
                do_sample=True,
                repetition_penalty=1.2,
                eos_token_id=tokenizer.eos_token_id,
                pad_token_id=tokenizer.pad_token_id if tokenizer.pad_token_id else tokenizer.eos_token_id,
                no_repeat_ngram_size=3,  # é¿å…é‡å¤çš„3-gram
            )
        
        # åªè§£ç ç”Ÿæˆçš„éƒ¨åˆ†ï¼ˆä» input_length å¼€å§‹ï¼‰
        generated_ids = outputs[0][input_length:]
        
        # è°ƒè¯•ï¼šæ£€æŸ¥ç”Ÿæˆçš„ token æ•°é‡
        if len(generated_ids) == 0:
            print(f"  âš  ç”Ÿæˆäº†0ä¸ªtokenï¼ˆoutput_length={len(outputs[0])}, input_length={input_length}ï¼‰")
            return None
        
        # è¿‡æ»¤æ‰ EOS å’Œ PAD token
        generated_ids_filtered = [
            token_id for token_id in generated_ids 
            if token_id not in [tokenizer.eos_token_id, tokenizer.pad_token_id]
        ]
        
        if len(generated_ids_filtered) == 0:
            print(f"  âš  ç”Ÿæˆçš„æ‰€æœ‰tokenéƒ½æ˜¯ç‰¹æ®Štokenï¼ˆEOS/PADï¼‰")
            return None
        
        generated_text = tokenizer.decode(generated_ids_filtered, skip_special_tokens=True).strip()
        
        # å¦‚æœç”Ÿæˆçš„å†…å®¹ä¸ºç©ºï¼Œè¿”å› None
        if not generated_text:
            # å°è¯•ä¸è§£ç ç”Ÿæˆçš„æ‰€æœ‰ token çœ‹çœ‹
            all_text = tokenizer.decode(generated_ids, skip_special_tokens=False)
            print(f"  âš  ç”Ÿæˆäº†ç©ºå†…å®¹ï¼ˆåŸå§‹è§£ç : {all_text[:50]}...ï¼‰")
            return None
        
        return generated_text
        
    except Exception as e:
        error_msg = str(e)
        error_type = type(e).__name__
        print(f"  âœ— æœ¬åœ°æ¨¡å‹ç”Ÿæˆå¤±è´¥")
        print(f"     é”™è¯¯ç±»å‹: {error_type}")
        print(f"     é”™è¯¯ä¿¡æ¯: {error_msg[:200]}...")  # åªæ˜¾ç¤ºå‰200ä¸ªå­—ç¬¦
        import traceback
        traceback.print_exc()
        return None


def load_local_model(model_name: str, device: str = "cuda"):
    """
    åŠ è½½æœ¬åœ°æ¨¡å‹å’Œåˆ†è¯å™¨
    
    Args:
        model_name: æ¨¡å‹åç§°
        device: è®¾å¤‡ï¼ˆcuda/cpuï¼‰
        
    Returns:
        (model, tokenizer, device) å…ƒç»„
    """
    print(f"  æ­£åœ¨åŠ è½½æ¨¡å‹ {model_name} åˆ°è®¾å¤‡ {device}...")
    
    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
    
    # è®¾ç½® pad token
    if tokenizer.pad_token is None:
        tokenizer.pad_token = tokenizer.eos_token
    
    # ç¡®å®š dtype
    if device == "cuda":
        dtype = torch.float16
    else:
        dtype = torch.float32
    
    model = AutoModelForCausalLM.from_pretrained(
        model_name,
        dtype=dtype,  # ä½¿ç”¨ dtype æ›¿ä»£ torch_dtypeï¼ˆtorch_dtype å·²å¼ƒç”¨ï¼‰
        device_map="auto" if device == "cuda" else None,
        trust_remote_code=True,
    )
    
    if device == "cpu":
        model = model.to(device)
    
    print(f"  âœ“ æ¨¡å‹åŠ è½½æˆåŠŸ")
    
    return model, tokenizer, device


def print_memory_info(device):
    """Print GPU memory information"""
    if device.type == "cuda":
        print("\n" + "=" * 60)
        print("GPU Memory Information")
        print("=" * 60)
        allocated = torch.cuda.memory_allocated(device) / 1e9
        reserved = torch.cuda.memory_reserved(device) / 1e9
        total = torch.cuda.get_device_properties(device).total_memory / 1e9
        print(f"Allocated: {allocated:.2f}GB")
        print(f"Reserved: {reserved:.2f}GB")
        print(f"Total GPU: {total:.2f}GB")
        print(f"Available: {(total - reserved):.2f}GB")
        print("=" * 60 + "\n")


def evaluate_icl(
    model_name: str = "Qwen/Qwen3-1.7B-Base",
    max_new_tokens: int = 1024,
    temperature: float = 0.7,
    top_p: float = 0.9,
    max_samples: Optional[int] = 10,  # é»˜è®¤åªè·‘10ä¸ªæ ·æœ¬ï¼ˆç”¨äºæµ‹è¯•ï¼‰ï¼ŒNoneè¡¨ç¤ºå…¨éƒ¨
    results_volume_obj=None,
):
    """
    åœ¨æµ‹è¯•é›†ä¸Šè¯„ä¼° ICL æ–¹æ³•
    
    Args:
        model_name: æ¨¡å‹åç§°
        max_new_tokens: æœ€å¤§ç”Ÿæˆtokenæ•°
        temperature: é‡‡æ ·æ¸©åº¦
        top_p: Nucleus samplingå‚æ•°
        max_samples: æœ€å¤§è¯„ä¼°æ ·æœ¬æ•°ï¼ˆ10ä¸ºé»˜è®¤æµ‹è¯•æ•°é‡ï¼ŒNoneè¡¨ç¤ºå…¨éƒ¨ï¼‰
        results_volume_obj: Modal volumeå¯¹è±¡ï¼Œç”¨äºä¿å­˜ç»“æœ
        
    Returns:
        è¯„ä¼°ç»“æœæ‘˜è¦
    """
    # åœ¨ Modal GPU ä¸Šæ€»æ˜¯ä½¿ç”¨ CUDA
    # åœ¨ Modal ä¸Šï¼Œå¦‚æœé…ç½®äº† GPUï¼Œåˆ™ä½¿ç”¨ CUDA
    device_str = "cuda" if torch.cuda.is_available() else "cpu"
    device = torch.device(device_str)
    
    print("=" * 60)
    print("ICL æ–¹æ³•è¯„ä¼° - æµ‹è¯•é›† (Modal)")
    print("=" * 60)
    print(f"æ¨¡å‹: {model_name}")
    print(f"è®¾å¤‡: {device}")
    print(f"æœ€å¤§æ ·æœ¬æ•°: {max_samples or 'å…¨éƒ¨'}")
    
    # åŠ è½½æµ‹è¯•é›†
    print("\nåŠ è½½æµ‹è¯•é›†...")
    dataset_path = "/dataset/hierarchical_dataset_clean"
    
    # æ£€æŸ¥æ•°æ®é›†æ˜¯å¦å­˜åœ¨
    import os
    if not os.path.exists(dataset_path):
        error_msg = f"""
âœ— é”™è¯¯: æ•°æ®é›†è·¯å¾„ä¸å­˜åœ¨: {dataset_path}

è¯·å…ˆä¸Šä¼ æ•°æ®é›†åˆ° Modal Volumeã€‚ä½ å¯ä»¥ï¼š

1. ä»æœ¬åœ°æ•°æ®é›†ä¸Šä¼ ï¼š
   modal volume put medical-dataset-volume \\
     ./data/processed/hierarchical_dataset_clean \\
     hierarchical_dataset_clean

2. æˆ–è€…æ£€æŸ¥æ•°æ®é›†æ˜¯å¦åœ¨ volume ä¸­çš„å…¶ä»–è·¯å¾„ï¼š
   modal volume ls medical-dataset-volume
"""
        print(error_msg)
        raise FileNotFoundError(f"Dataset not found at {dataset_path}. Please upload it first.")
    
    # ä»å®Œæ•´æ•°æ®é›†åŠ è½½
    print(f"ä» {dataset_path} åŠ è½½æ•°æ®é›†...")
    dataset = load_from_disk(dataset_path)
    print(f"âœ“ æ•°æ®é›†åŠ è½½æˆåŠŸï¼Œå…± {len(dataset)} ä¸ªæ ·æœ¬")
    
    # ä½¿ç”¨ç›¸åŒçš„åˆ’åˆ†æ–¹å¼
    print("åˆ’åˆ†è®­ç»ƒé›†å’Œæµ‹è¯•é›†ï¼ˆtest_size=0.1, seed=42ï¼‰...")
    split_dataset = dataset.train_test_split(test_size=0.1, seed=42)
    test_dataset = split_dataset["test"]
    
    print(f"âœ“ æµ‹è¯•é›†åŠ è½½æˆåŠŸï¼Œå…± {len(test_dataset)} ä¸ªæ ·æœ¬")
    
    # ç¡®å®šè¯„ä¼°æ ·æœ¬æ•°
    if max_samples is None:
        num_samples = len(test_dataset)
    else:
        num_samples = min(max_samples, len(test_dataset))
    print(f"\nå°†è¯„ä¼° {num_samples} ä¸ªæ ·æœ¬ï¼ˆå…± {len(test_dataset)} ä¸ªï¼‰")
    
    # åŠ è½½æ¨¡å‹
    print(f"\n{'='*60}")
    print("åŠ è½½æ¨¡å‹")
    print(f"{'='*60}")
    model, tokenizer, device_str = load_local_model(model_name, device_str)
    device = torch.device(device_str)
    
    print_memory_info(device)
    
    # åˆ›å»ºè¾“å‡ºç›®å½•
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    output_dir = f"/results/icl_eval_{timestamp}"
    os.makedirs(output_dir, exist_ok=True)
    
    results = []
    failed_count = 0
    
    print(f"\n{'='*60}")
    print("å¼€å§‹è¯„ä¼°")
    print(f"{'='*60}\n")
    
    for i in range(num_samples):
        sample = test_dataset[i]
        
        # æ„å»ºé—®é¢˜ï¼ˆä½¿ç”¨ Description ä½œä¸ºé—®é¢˜ï¼‰
        question = sample['Description']
        
        print(f"\n[{i+1}/{num_samples}] å¤„ç†é—®é¢˜...")
        print(f"é—®é¢˜: {question[:100]}..." if len(question) > 100 else f"é—®é¢˜: {question}")
        
        # ä½¿ç”¨ ICL ç”Ÿæˆç­”æ¡ˆ
        prompt = create_prompt(question)
        
        start_time = time.time()
        generated_answer = call_local_model(
            model,
            tokenizer,
            device,
            prompt,
            max_new_tokens=max_new_tokens,
            temperature=temperature,
            top_p=top_p,
        )
        elapsed_time = time.time() - start_time
        
        if generated_answer:
            print(f"  âœ“ ç”ŸæˆæˆåŠŸ (è€—æ—¶: {elapsed_time:.2f}s)")
            print(f"  ç­”æ¡ˆé•¿åº¦: {len(generated_answer)} å­—ç¬¦")
        else:
            print(f"  âœ— ç”Ÿæˆå¤±è´¥")
            failed_count += 1
        
        # ä¿å­˜ç»“æœ
        result = {
            "index": i,
            "question": question,
            "patient_description": sample.get('Patient', ''),
            "reference_answer": sample.get('Doctor', ''),
            "generated_answer": generated_answer if generated_answer else "FAILED",
            "status": sample.get('Status', ''),
            "generation_time": elapsed_time,
            "success": generated_answer is not None,
        }
        results.append(result)
        
        # æ¯10ä¸ªæ ·æœ¬ä¿å­˜ä¸€æ¬¡ä¸­é—´ç»“æœ
        if (i + 1) % 10 == 0:
            checkpoint_file = os.path.join(output_dir, f"icl_results_checkpoint_{i+1}.json")
            with open(checkpoint_file, 'w', encoding='utf-8') as f:
                json.dump(results, f, indent=2, ensure_ascii=False)
            print(f"  ğŸ’¾ å·²ä¿å­˜æ£€æŸ¥ç‚¹: {checkpoint_file}")
            
            # æäº¤ volume æ›´æ–°
            if results_volume_obj:
                results_volume_obj.commit()
    
    # ä¿å­˜æœ€ç»ˆç»“æœ
    results_file = os.path.join(output_dir, f"icl_results_{timestamp}.json")
    
    with open(results_file, 'w', encoding='utf-8') as f:
        json.dump(results, f, indent=2, ensure_ascii=False)
    
    print(f"\n{'='*60}")
    print("è¯„ä¼°å®Œæˆ")
    print(f"{'='*60}")
    print(f"æ€»æ ·æœ¬æ•°: {num_samples}")
    print(f"æˆåŠŸ: {num_samples - failed_count}")
    print(f"å¤±è´¥: {failed_count}")
    print(f"æˆåŠŸç‡: {(num_samples - failed_count) / num_samples * 100:.1f}%")
    print(f"\nç»“æœå·²ä¿å­˜åˆ°: {results_file}")
    
    # ç”Ÿæˆæ‘˜è¦
    summary = {
        "model": model_name,
        "total_samples": num_samples,
        "successful": num_samples - failed_count,
        "failed": failed_count,
        "success_rate": (num_samples - failed_count) / num_samples * 100,
        "average_generation_time": sum(r['generation_time'] for r in results) / len(results) if results else 0,
        "results_file": results_file,
        "timestamp": timestamp,
        "device": device_str,
    }
    
    summary_file = os.path.join(output_dir, f"icl_summary_{timestamp}.json")
    with open(summary_file, 'w', encoding='utf-8') as f:
        json.dump(summary, f, indent=2, ensure_ascii=False)
    
    print(f"æ‘˜è¦å·²ä¿å­˜åˆ°: {summary_file}")
    
    # æäº¤ volume æ›´æ–°
    if results_volume_obj:
        results_volume_obj.commit()
    
    print_memory_info(device)
    
    return summary


# Create Modal app and volumes
app = modal.App("medical-icl-evaluation")

# Create volumes for dataset and results
dataset_volume = Volume.from_name("medical-dataset-volume", create_if_missing=True)
results_volume = Volume.from_name("medical-results-volume", create_if_missing=True)

# Docker image with necessary dependencies
image = modal.Image.debian_slim().pip_install(
    "torch",
    "transformers",
    "datasets",
    "accelerate",
)


@app.function(
    image=image,
    gpu="A10",  # å¯ä»¥æ”¹ä¸º None ä½¿ç”¨ CPUï¼Œæˆ– "A10G" ä½¿ç”¨æ›´ä¾¿å®œçš„ GPUï¼Œæˆ– "T4" ä½¿ç”¨æ›´ä¾¿å®œçš„ GPU
    volumes={
        "/dataset": dataset_volume,
        "/results": results_volume,
    },
    timeout=86400,  # 24å°æ—¶è¶…æ—¶
)
def evaluate(
    model: str = "Qwen/Qwen3-1.7B-Base",
    max_new_tokens: int = 1024,
    temperature: float = 0.7,
    top_p: float = 0.9,
    max_samples: Optional[int] = 10,  # é»˜è®¤åªè·‘10ä¸ªæ ·æœ¬ï¼ˆç”¨äºæµ‹è¯•ï¼‰ï¼Œå¯æ”¹ä¸º None è·‘å…¨éƒ¨æˆ–æŒ‡å®šæ•°é‡
):
    """
    Entrypoint for modal run command
    
    Example:
        # é»˜è®¤è·‘10ä¸ªæ ·æœ¬ï¼ˆæµ‹è¯•ç”¨ï¼‰
        modal run scripts/modal_qwen_1.78B_base_evaluate_icl.py::evaluate --model "Qwen/Qwen2.5-0.5B"
        
        # è·‘50ä¸ªæ ·æœ¬
        modal run scripts/modal_qwen_1.78B_base_evaluate_icl.py::evaluate --model "Qwen/Qwen2.5-0.5B" --max-samples 50
        
        # è·‘å…¨éƒ¨332ä¸ªæµ‹è¯•æ ·æœ¬
        modal run scripts/modal_qwen_1.78B_base_evaluate_icl.py::evaluate --model "Qwen/Qwen2.5-0.5B" --max-samples 332
    """
    # åœ¨å‡½æ•°å†…éƒ¨è®¿é—® volume
    return evaluate_icl(
        model_name=model,
        max_new_tokens=max_new_tokens,
        temperature=temperature,
        top_p=top_p,
        max_samples=max_samples,
        results_volume_obj=results_volume,
    )

