"""
Modal script for evaluating ICL method on test set
- Loads dataset from volume
- Uses local model loading (not Inference API)
- Saves results to volume
- Supports GPU (A10) or CPU
- é»˜è®¤è¯„ä¼°æµ‹è¯•é›†å…¨éƒ¨æ ·æœ¬ï¼ˆæµ‹è¯•é›†çº¦10%çš„æ•°æ®ï¼‰

Usage:
  # ä½¿ç”¨é»˜è®¤æ¨¡å‹ (TinyLlama 1.1B) - è¯„ä¼°æµ‹è¯•é›†å…¨éƒ¨æ ·æœ¬
  modal run scripts/modal_llama_3_1B_base_icl.py::evaluate
  
  # ä½¿ç”¨å…¶ä»– 1B æ¨¡å‹ - è¯„ä¼°æµ‹è¯•é›†
  modal run scripts/modal_llama_3_1B_base_icl.py::evaluate --model "TinyLlama/TinyLlama-1.1B-intermediate-step-1431k-3T"
  
  # è¯„ä¼°ä¸åŒæ•°é‡çš„æµ‹è¯•é›†æ ·æœ¬
  modal run scripts/modal_llama_3_1B_base_icl.py::evaluate --max-samples 50
  modal run scripts/modal_llama_3_1B_base_icl.py::evaluate --max-samples 100
  
  å¯ç”¨çš„ 1B çº§åˆ«æ¨¡å‹ï¼ˆé Qwen/DeepSeekï¼‰:
  - TinyLlama/TinyLlama-1.1B-Chat-v1.0 (æ¨èï¼Œchatç‰ˆæœ¬)
  - TinyLlama/TinyLlama-1.1B-intermediate-step-1431k-3T (baseç‰ˆæœ¬)
"""

import os
import json
import time
import torch
from pathlib import Path
from datetime import datetime
from typing import Dict, List, Optional
from transformers import AutoTokenizer, AutoModelForCausalLM
from datasets import load_from_disk
import modal
from modal import Volume, Secret


# ICL Template
ICL_TEMPLATE = """Task: Transform the medical/nutrition question into a MECE hierarchical answer.

=== OUTPUT FORMAT (STRICT) ===
- **1. <Top-level section>**
  - **1.1 <Subsection>**
    - <bullet>
    - <bullet>
- **2. <Top-level section>**
  ...
Rules:
A) Use MECE structure in each level.
B) No diagnosis; only plausible causes + safe advice.
C) No extra text before/after hierarchy.

=== EXAMPLE ===
Input: hi doctor my mom aged 45 years is having a fever for almost two months the temperature used to vary from 100 degrees celsius in the night times and gradually it comes down she was weighing 105 keg and now she is keg now she is having bone pain and light fever in the night when there is a pain in the bone she has been advised with calcium and iron tablets she is also getting her regular periods menopause has not reached yet kindly advise
Output:
- **Clinical Presentation Suggestive of Tuberculosis**
  - **Chronic Fever Pattern**
    - Duration of two months
    - Low-grade fever, especially in evenings
  - **Associated Systemic Symptoms**
    - Unexplained weight loss
    - Bone pains
- **Recommended Diagnostic Investigations**
  - **Specific Tests for Tuberculosis**
    - Mantoux test (tuberculin skin test)
    - Adenosine deaminase (ADA) levels
  - **Purpose of Testing**
    - To confirm or rule out tuberculosis infection
- **Epidemiological Risk Assessment**
  - **Residence in Colder Country**
    - Lower tuberculosis prevalence reduces likelihood
  - **Recent Travel/Exposure History**
    - Staying in India for >6 months increases risk
    - Recent visit to India (2 months ago) suggests possible infection acquisition
- **Supportive Management Measures**
  - **Hydration and Nutrition**
    - Maintain adequate fluid intake
    - Daily multivitamin supplementation
  - **Appetite Enhancement**
    - Address probable appetite loss over past two months
- **Follow-up Protocol**
  - **Next Steps**
    - Complete recommended diagnostic tests
    - Share laboratory reports for further evaluation
    
=== NOW DO THIS ===
Input: {{question}}
Output:"""

def create_prompt(patient_description: str) -> str:
    """åˆ›å»º ICL promptï¼Œä½¿ç”¨æ‚£è€…æè¿°ï¼ˆPatientå­—æ®µï¼‰ä½œä¸ºè¾“å…¥"""
    # æ³¨æ„ï¼šICLæ¨¡æ¿ä¸­ä½¿ç”¨ {{question}} ä½œä¸ºå ä½ç¬¦ï¼Œä½†å®é™…ä¼ å…¥çš„æ˜¯ Patient å­—æ®µï¼ˆæ‚£è€…è¯¦ç»†æè¿°ï¼‰
    return ICL_TEMPLATE.replace("{{question}}", patient_description)


def call_local_model(
    model,
    tokenizer,
    device,
    prompt: str,
    max_new_tokens: int = 1024,
    temperature: float = 0.7,
    top_p: float = 0.9,
) -> Optional[str]:
    """
    ä½¿ç”¨æœ¬åœ°åŠ è½½çš„æ¨¡å‹ç”Ÿæˆå“åº”
    
    Args:
        model: æœ¬åœ°åŠ è½½çš„æ¨¡å‹
        tokenizer: åˆ†è¯å™¨
        device: è®¾å¤‡ï¼ˆcuda/cpuï¼‰
        prompt: è¾“å…¥æç¤º
        max_new_tokens: æœ€å¤§ç”Ÿæˆtokenæ•°
        temperature: é‡‡æ ·æ¸©åº¦
        top_p: Nucleus samplingå‚æ•°
        
    Returns:
        ç”Ÿæˆçš„å“åº”æ–‡æœ¬æˆ– Noneï¼ˆå¦‚æœå¤±è´¥ï¼‰
    """
    try:
        inputs = tokenizer(prompt, return_tensors="pt").to(device)
        input_length = inputs.input_ids.shape[1]  # è®°å½• prompt çš„é•¿åº¦
        
        # æ£€æŸ¥è¾“å…¥é•¿åº¦ (Qwen2.5 æ”¯æŒ 32k context)
        if input_length > 30000:
            print(f"  âš  è¾“å…¥ Prompt æé•¿ ({input_length} tokens)")
        
        model.eval()
        
        with torch.no_grad():
            outputs = model.generate(
                **inputs,
                max_new_tokens=max_new_tokens,
                min_new_tokens=10,  # ç¡®ä¿è‡³å°‘ç”Ÿæˆ10ä¸ªtoken
                temperature=temperature,
                top_p=top_p,
                do_sample=True,
                repetition_penalty=1.2,
                eos_token_id=tokenizer.eos_token_id,
                pad_token_id=tokenizer.pad_token_id if tokenizer.pad_token_id else tokenizer.eos_token_id,
                no_repeat_ngram_size=3,  # é¿å…é‡å¤çš„3-gram
            )
        
        # åªè§£ç ç”Ÿæˆçš„éƒ¨åˆ†ï¼ˆä» input_length å¼€å§‹ï¼‰
        generated_ids = outputs[0][input_length:]
        
        # è°ƒè¯•ï¼šæ£€æŸ¥ç”Ÿæˆçš„ token æ•°é‡
        if len(generated_ids) == 0:
            print(f"  âš  ç”Ÿæˆäº†0ä¸ªtokenï¼ˆoutput_length={len(outputs[0])}, input_length={input_length}ï¼‰")
            return None
        
        # è¿‡æ»¤æ‰ EOS å’Œ PAD token
        generated_ids_filtered = [
            token_id for token_id in generated_ids 
            if token_id not in [tokenizer.eos_token_id, tokenizer.pad_token_id]
        ]
        
        if len(generated_ids_filtered) == 0:
            print(f"  âš  ç”Ÿæˆçš„æ‰€æœ‰tokenéƒ½æ˜¯ç‰¹æ®Štokenï¼ˆEOS/PADï¼‰")
            return None
        
        generated_text = tokenizer.decode(generated_ids_filtered, skip_special_tokens=True).strip()
        
        # å¦‚æœç”Ÿæˆçš„å†…å®¹ä¸ºç©ºï¼Œè¿”å› None
        if not generated_text:
            # å°è¯•ä¸è§£ç ç”Ÿæˆçš„æ‰€æœ‰ token çœ‹çœ‹
            all_text = tokenizer.decode(generated_ids, skip_special_tokens=False)
            print(f"  âš  ç”Ÿæˆäº†ç©ºå†…å®¹ï¼ˆåŸå§‹è§£ç : {all_text[:50]}...ï¼‰")
            return None
        
        return generated_text
        
    except Exception as e:
        error_msg = str(e)
        error_type = type(e).__name__
        print(f"  âœ— æœ¬åœ°æ¨¡å‹ç”Ÿæˆå¤±è´¥")
        print(f"     é”™è¯¯ç±»å‹: {error_type}")
        print(f"     é”™è¯¯ä¿¡æ¯: {error_msg[:200]}...")  # åªæ˜¾ç¤ºå‰200ä¸ªå­—ç¬¦
        import traceback
        traceback.print_exc()
        return None


def load_local_model(model_name: str, device: str = "cuda", hf_token: Optional[str] = None):
    """
    åŠ è½½æœ¬åœ°æ¨¡å‹å’Œåˆ†è¯å™¨
    
    Args:
        model_name: æ¨¡å‹åç§°
        device: è®¾å¤‡ï¼ˆcuda/cpuï¼‰
        hf_token: Hugging Face tokenï¼ˆç”¨äºè®¿é—® gated æ¨¡å‹å¦‚ Llamaï¼‰
        
    Returns:
        (model, tokenizer, device) å…ƒç»„
    """
    print(f"  æ­£åœ¨åŠ è½½æ¨¡å‹ {model_name} åˆ°è®¾å¤‡ {device}...")
    
    # è·å– HF tokenï¼ˆç”¨äº gated æ¨¡å‹å¦‚ Llamaï¼‰
    if hf_token is None:
        hf_token = os.environ.get("HF_TOKEN") or os.environ.get("HUGGING_FACE_HUB_TOKEN")
    
    # å‡†å¤‡ tokenizer å‚æ•°
    tokenizer_kwargs = {}
    if hf_token:
        tokenizer_kwargs["token"] = hf_token
        print(f"  âœ“ ä½¿ç”¨ Hugging Face token è®¿é—®æ¨¡å‹")
    
    # Llama æ¨¡å‹é€šå¸¸ä¸éœ€è¦ trust_remote_codeï¼Œä½† Qwen éœ€è¦ï¼Œæ‰€ä»¥ä¿ç•™ä»¥å…¼å®¹ä¸¤ç§æ¨¡å‹
    try:
        tokenizer = AutoTokenizer.from_pretrained(
            model_name, 
            trust_remote_code=True,
            **tokenizer_kwargs
        )
    except Exception as e1:
        # å¦‚æœå¤±è´¥ï¼Œå°è¯•ä¸ä½¿ç”¨ trust_remote_codeï¼ˆé€‚ç”¨äº Llama æ¨¡å‹ï¼‰
        try:
            tokenizer = AutoTokenizer.from_pretrained(
                model_name,
                **tokenizer_kwargs
            )
        except Exception as e2:
            print(f"  âœ— Tokenizer åŠ è½½å¤±è´¥")
            print(f"     é”™è¯¯1: {str(e1)[:200]}")
            print(f"     é”™è¯¯2: {str(e2)[:200]}")
            raise
    
    # è®¾ç½® pad tokenï¼ˆLlama æ¨¡å‹é€šå¸¸éœ€è¦è¿™ä¸ªè®¾ç½®ï¼‰
    if tokenizer.pad_token is None:
        tokenizer.pad_token = tokenizer.eos_token
    
    # ç¡®å®š dtype
    if device == "cuda":
        dtype = torch.float16
    else:
        dtype = torch.float32
    
    # å‡†å¤‡ model å‚æ•°
    model_kwargs = {
        "dtype": dtype,
        "device_map": "auto" if device == "cuda" else None,
    }
    if hf_token:
        model_kwargs["token"] = hf_token
    
    # Llama æ¨¡å‹é€šå¸¸ä¸éœ€è¦ trust_remote_codeï¼Œä½† Qwen éœ€è¦ï¼Œæ‰€ä»¥ä¿ç•™ä»¥å…¼å®¹ä¸¤ç§æ¨¡å‹
    try:
        model = AutoModelForCausalLM.from_pretrained(
            model_name,
            trust_remote_code=True,
            **model_kwargs
        )
    except Exception:
        # å¦‚æœå¤±è´¥ï¼Œå°è¯•ä¸ä½¿ç”¨ trust_remote_codeï¼ˆé€‚ç”¨äº Llama æ¨¡å‹ï¼‰
        model = AutoModelForCausalLM.from_pretrained(
            model_name,
            **model_kwargs
        )
    
    if device == "cpu":
        model = model.to(device)
    
    print(f"  âœ“ æ¨¡å‹åŠ è½½æˆåŠŸ")
    
    return model, tokenizer, device


def print_memory_info(device):
    """Print GPU memory information"""
    if device.type == "cuda":
        print("\n" + "=" * 60)
        print("GPU Memory Information")
        print("=" * 60)
        allocated = torch.cuda.memory_allocated(device) / 1e9
        reserved = torch.cuda.memory_reserved(device) / 1e9
        total = torch.cuda.get_device_properties(device).total_memory / 1e9
        print(f"Allocated: {allocated:.2f}GB")
        print(f"Reserved: {reserved:.2f}GB")
        print(f"Total GPU: {total:.2f}GB")
        print(f"Available: {(total - reserved):.2f}GB")
        print("=" * 60 + "\n")


def evaluate_icl(
    model_name: str = "Qwen/Qwen2.5-1.5B",  # Qwen2.5 1.5B Base
    max_new_tokens: int = 1024,
    temperature: float = 0.7,
    top_p: float = 0.9,
    max_samples: Optional[int] = None,  # é»˜è®¤è¯„ä¼°æµ‹è¯•é›†å…¨éƒ¨æ ·æœ¬ï¼Œå¯æŒ‡å®šå…¶ä»–æ•°é‡
    hf_token: Optional[str] = None,  # Hugging Face tokenï¼ˆç”¨äºè®¿é—® gated æ¨¡å‹ï¼‰
    results_volume_obj=None,
):
    """
    åœ¨æµ‹è¯•é›†ä¸Šè¯„ä¼° ICL æ–¹æ³•ï¼ˆé»˜è®¤è¯„ä¼°å…¨éƒ¨æµ‹è¯•é›†æ ·æœ¬ï¼‰
    
    Args:
        model_name: æ¨¡å‹åç§°
        max_new_tokens: æœ€å¤§ç”Ÿæˆtokenæ•°
        temperature: é‡‡æ ·æ¸©åº¦
        top_p: Nucleus samplingå‚æ•°
        max_samples: è¯„ä¼°æ ·æœ¬æ•°ï¼ˆé»˜è®¤Noneè¡¨ç¤ºå…¨éƒ¨æµ‹è¯•é›†æ ·æœ¬ï¼Œå¯æŒ‡å®šå…¶ä»–æ•°é‡ï¼‰
        results_volume_obj: Modal volumeå¯¹è±¡ï¼Œç”¨äºä¿å­˜ç»“æœ
        
    Returns:
        è¯„ä¼°ç»“æœæ‘˜è¦
    """
    # åœ¨ Modal GPU ä¸Šæ€»æ˜¯ä½¿ç”¨ CUDA
    # åœ¨ Modal ä¸Šï¼Œå¦‚æœé…ç½®äº† GPUï¼Œåˆ™ä½¿ç”¨ CUDA
    device_str = "cuda" if torch.cuda.is_available() else "cpu"
    device = torch.device(device_str)
    
    print("=" * 60)
    print("ICL æ–¹æ³•è¯„ä¼° - æµ‹è¯•é›† (Modal) - Qwen2.5-1.5B")
    print("=" * 60)
    print(f"æ¨¡å‹: {model_name}")
    print(f"è®¾å¤‡: {device}")
    print(f"è¯„ä¼°æ ·æœ¬æ•°: {max_samples or 'å…¨éƒ¨æµ‹è¯•é›†'}")
    
    # åŠ è½½æ•°æ®é›†
    print("\nåŠ è½½æ•°æ®é›†...")
    dataset_path = "/dataset/hierarchical_dataset_clean"
    
    # æ£€æŸ¥æ•°æ®é›†æ˜¯å¦å­˜åœ¨
    import os
    if not os.path.exists(dataset_path):
        error_msg = f"""
âœ— é”™è¯¯: æ•°æ®é›†è·¯å¾„ä¸å­˜åœ¨: {dataset_path}

è¯·å…ˆä¸Šä¼ æ•°æ®é›†åˆ° Modal Volumeã€‚ä½ å¯ä»¥ï¼š

1. ä»æœ¬åœ°æ•°æ®é›†ä¸Šä¼ ï¼š
   modal volume put medical-dataset-volume \\
     ./data/processed/hierarchical_dataset_clean \\
     hierarchical_dataset_clean

2. æˆ–è€…æ£€æŸ¥æ•°æ®é›†æ˜¯å¦åœ¨ volume ä¸­çš„å…¶ä»–è·¯å¾„ï¼š
   modal volume ls medical-dataset-volume
"""
        print(error_msg)
        raise FileNotFoundError(f"Dataset not found at {dataset_path}. Please upload it first.")
    
    # ä»å®Œæ•´æ•°æ®é›†åŠ è½½
    print(f"ä» {dataset_path} åŠ è½½æ•°æ®é›†...")
    dataset = load_from_disk(dataset_path)
    print(f"âœ“ æ•°æ®é›†åŠ è½½æˆåŠŸï¼Œå…± {len(dataset)} ä¸ªæ ·æœ¬")
    
    # ä½¿ç”¨ç›¸åŒçš„åˆ’åˆ†æ–¹å¼
    print("åˆ’åˆ†è®­ç»ƒé›†å’Œæµ‹è¯•é›†ï¼ˆtest_size=0.1, seed=42ï¼‰...")
    split_dataset = dataset.train_test_split(test_size=0.1, seed=42)
    test_dataset = split_dataset["test"]
    
    print(f"âœ“ æµ‹è¯•é›†åŠ è½½æˆåŠŸï¼Œå…± {len(test_dataset)} ä¸ªæ ·æœ¬")
    
    # ç¡®å®šè¯„ä¼°æ ·æœ¬æ•°
    if max_samples is None:
        num_samples = len(test_dataset)
    else:
        num_samples = min(max_samples, len(test_dataset))
    print(f"\nå°†ä»æµ‹è¯•é›†ä¸­è¯„ä¼° {num_samples} ä¸ªæ ·æœ¬ï¼ˆæµ‹è¯•é›†å…± {len(test_dataset)} ä¸ªï¼‰")
    
    # åŠ è½½æ¨¡å‹
    print(f"\n{'='*60}")
    print("åŠ è½½æ¨¡å‹")
    print(f"{'='*60}")
    model, tokenizer, device_str = load_local_model(model_name, device_str, hf_token=hf_token)
    device = torch.device(device_str)
    
    print_memory_info(device)
    
    # åˆ›å»ºè¾“å‡ºç›®å½•
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    output_dir = f"/results/icl_eval_test_qwen2_5_1_5b_{timestamp}"
    os.makedirs(output_dir, exist_ok=True)
    
    results = []
    failed_count = 0
    
    # æ¯20ä¸ªæ ·æœ¬ä¿å­˜ä¸€æ¬¡ä¸­é—´ç»“æœ
    checkpoint_interval = 20
    
    print(f"\n{'='*60}")
    print("å¼€å§‹è¯„ä¼°")
    print(f"{'='*60}\n")
    
    for i in range(num_samples):
        sample = test_dataset[i]
        
        # æ„å»ºé—®é¢˜ï¼ˆä½¿ç”¨ Patient ä½œä¸ºè¾“å…¥ï¼ŒåŒ…å«æ‚£è€…çš„è¯¦ç»†æè¿°ï¼‰
        patient_input = sample.get('Patient', '')
        if not patient_input:
            # å¦‚æœ Patient ä¸ºç©ºï¼Œå›é€€åˆ° Description
            patient_input = sample.get('Description', '')
        
        print(f"\n[{i+1}/{num_samples}] å¤„ç†é—®é¢˜...")
        print(f"æ‚£è€…æè¿°: {patient_input[:100]}..." if len(patient_input) > 100 else f"æ‚£è€…æè¿°: {patient_input}")
        
        # ä½¿ç”¨ ICL ç”Ÿæˆç­”æ¡ˆï¼ˆä¼ å…¥æ‚£è€…æè¿°ï¼‰
        prompt = create_prompt(patient_input)
        
        start_time = time.time()
        generated_answer = call_local_model(
            model,
            tokenizer,
            device,
            prompt,
            max_new_tokens=max_new_tokens,
            temperature=temperature,
            top_p=top_p,
        )
        elapsed_time = time.time() - start_time
        
        if generated_answer:
            print(f"  âœ“ ç”ŸæˆæˆåŠŸ (è€—æ—¶: {elapsed_time:.2f}s)")
            print(f"  ç­”æ¡ˆé•¿åº¦: {len(generated_answer)} å­—ç¬¦")
        else:
            print(f"  âœ— ç”Ÿæˆå¤±è´¥")
            failed_count += 1
        
        # ä¿å­˜ç»“æœ
        result = {
            "index": i,
            "description": sample.get('Description', ''),  # ç®€çŸ­çš„é—®é¢˜æè¿°
            "patient_description": patient_input,  # æ‚£è€…è¯¦ç»†æè¿°ï¼ˆç”¨äºICLè¾“å…¥ï¼‰
            "reference_answer": sample.get('Doctor', ''),
            "generated_answer": generated_answer if generated_answer else "FAILED",
            "status": sample.get('Status', ''),
            "generation_time": elapsed_time,
            "success": generated_answer is not None,
        }
        results.append(result)
        
        # æ¯20ä¸ªæ ·æœ¬ä¿å­˜ä¸€æ¬¡ä¸­é—´ç»“æœ
        if (i + 1) % checkpoint_interval == 0:
            checkpoint_file = os.path.join(output_dir, f"icl_results_checkpoint_{i+1}.json")
            with open(checkpoint_file, 'w', encoding='utf-8') as f:
                json.dump(results, f, indent=2, ensure_ascii=False)
            print(f"  ğŸ’¾ å·²ä¿å­˜æ£€æŸ¥ç‚¹ ({i+1}/{num_samples}): {checkpoint_file}")
            
            # æäº¤ volume æ›´æ–°
            if results_volume_obj:
                results_volume_obj.commit()
    
    # ä¿å­˜æœ€ç»ˆç»“æœ
    results_file = os.path.join(output_dir, f"icl_results_{timestamp}.json")
    
    with open(results_file, 'w', encoding='utf-8') as f:
        json.dump(results, f, indent=2, ensure_ascii=False)
    
    print(f"\n{'='*60}")
    print("è¯„ä¼°å®Œæˆ")
    print(f"{'='*60}")
    print(f"æ€»æ ·æœ¬æ•°: {num_samples}")
    print(f"æˆåŠŸ: {num_samples - failed_count}")
    print(f"å¤±è´¥: {failed_count}")
    print(f"æˆåŠŸç‡: {(num_samples - failed_count) / num_samples * 100:.1f}%")
    print(f"\nç»“æœå·²ä¿å­˜åˆ°: {results_file}")
    
    # ç”Ÿæˆæ‘˜è¦
    summary = {
        "model": model_name,
        "total_samples": num_samples,
        "successful": num_samples - failed_count,
        "failed": failed_count,
        "success_rate": (num_samples - failed_count) / num_samples * 100,
        "average_generation_time": sum(r['generation_time'] for r in results) / len(results) if results else 0,
        "results_file": results_file,
        "timestamp": timestamp,
        "device": device_str,
    }
    
    summary_file = os.path.join(output_dir, f"icl_summary_{timestamp}.json")
    with open(summary_file, 'w', encoding='utf-8') as f:
        json.dump(summary, f, indent=2, ensure_ascii=False)
    
    print(f"æ‘˜è¦å·²ä¿å­˜åˆ°: {summary_file}")
    
    # æäº¤ volume æ›´æ–°
    if results_volume_obj:
        results_volume_obj.commit()
    
    print_memory_info(device)
    
    return summary


# Create Modal app and volumes
app = modal.App("medical-icl-evaluation")

# Create volumes for dataset and results
dataset_volume = Volume.from_name("medical-dataset-volume", create_if_missing=True)
results_volume = Volume.from_name("medical-results-volume", create_if_missing=True)

# Docker image with necessary dependencies
image = modal.Image.debian_slim().pip_install(
    "torch",
    "transformers",
    "datasets",
    "accelerate",
)


@app.function(
    image=image,
    gpu="A10",  # å¯ä»¥æ”¹ä¸º None ä½¿ç”¨ CPUï¼Œæˆ– "A10G" ä½¿ç”¨æ›´ä¾¿å®œçš„ GPUï¼Œæˆ– "T4" ä½¿ç”¨æ›´ä¾¿å®œçš„ GPU
    volumes={
        "/dataset": dataset_volume,
        "/results": results_volume,
    },
    # secrets=[Secret.from_name("huggingface-secret")],  # Qwen2.5 1.5B ä¸éœ€è¦ gated access (é€šå¸¸)
    timeout=86400,  # 24å°æ—¶è¶…æ—¶
)
def evaluate(
    model: str = "Qwen/Qwen2.5-1.5B",  # Qwen2.5 1.5B Base
    max_new_tokens: int = 1024,
    temperature: float = 0.7,
    top_p: float = 0.9,
    max_samples: Optional[int] = 100,  # é»˜è®¤è¯„ä¼°æµ‹è¯•é›†å…¨éƒ¨æ ·æœ¬ï¼Œå¯æŒ‡å®šå…¶ä»–æ•°é‡
):
    """
    Entrypoint for modal run command - é»˜è®¤è¯„ä¼°æµ‹è¯•é›†å…¨éƒ¨æ ·æœ¬
    
    Example:
        # è¯„ä¼°æµ‹è¯•é›†å…¨éƒ¨æ ·æœ¬ - é»˜è®¤è¡Œä¸º (Qwen2.5-1.5B)
        modal run scripts/modal_llama_3_1B_base_icl.py::evaluate
        
        # ä½¿ç”¨å…¶ä»–æ¨¡å‹ï¼š
        modal run scripts/modal_llama_3_1B_base_icl.py::evaluate --model "TinyLlama/TinyLlama-1.1B-Chat-v1.0"
        
        å¯ç”¨çš„ 1B æ¨¡å‹é€‰é¡¹ï¼š
        - Qwen/Qwen2.5-1.5B (1.5B, 32k context, æ¨è)
        - TinyLlama/TinyLlama-1.1B-Chat-v1.0 (1.1B, 2k context)
        
        # è¯„ä¼°ä¸åŒæ•°é‡çš„æµ‹è¯•é›†æ ·æœ¬
        modal run scripts/modal_llama_3_1B_base_icl.py::evaluate --max-samples 50
    """
    # ä»ç¯å¢ƒå˜é‡è·å– HF tokenï¼ˆé€šè¿‡ Modal secretï¼‰
    hf_token = os.environ.get("HF_TOKEN") or os.environ.get("HUGGING_FACE_HUB_TOKEN")
    
    # åœ¨å‡½æ•°å†…éƒ¨è®¿é—® volume
    return evaluate_icl(
        model_name=model,
        max_new_tokens=max_new_tokens,
        temperature=temperature,
        top_p=top_p,
        max_samples=max_samples,
        hf_token=hf_token,
        results_volume_obj=results_volume,
    )

