\documentclass{article}

\usepackage[margin=1in]{geometry}
\usepackage{graphicx}
\usepackage{fancyvrb}
\usepackage{fvextra}
\usepackage{xcolor}
\definecolor{mauve}{rgb}{0.58,0,0.82}
\usepackage{titlesec}
\usepackage[scaled=0.96]{helvet} % Sets up Helvetica font
\usepackage{amsmath}
\usepackage{listings}
\usepackage{hyperref}
\usepackage{booktabs}
\usepackage{float}
\usepackage{tcolorbox}
\tcbuselibrary{skins,breakable}

\newtcolorbox{promptbox}[2][]{%
  breakable,
  colback=gray!5,
  colframe=gray!60,
  boxrule=0.5pt,
  arc=1mm,
  left=1mm, right=1mm, top=1mm, bottom=1mm,
  title={#2},    % 标题文本
  #1             % 其他可选参数
}
\titleformat{\section}[hang]{\color{mauve}\normalfont\sffamily\Large}{\thesection}{1em}{}
\titleformat{\subsection}[hang]{\color{mauve}\normalfont\sffamily\large}{\thesubsection}{1em}{}

\title{Mini Project}
\author{Xinshi Wang (xinshiw), Shawn Xiang (zhengxia)}

\begin{document}
\maketitle
\section{Problem 1: Picking a Task and a Dataset}
\subsection{Task Description and Motivation} 
We propose generating medical and nutrition answers in MECE-structured hierarchical form from patient questions using small LLM , which improves readability and downstream automation.\\
This task is interesting because it requires structured generation, medical reasoning, and information organization without performing diagnosis. Compared with standard summarization or QA, our task forces models to (1) identify relevant clinical/nutritional factors, (2) group them into non-overlapping categories, and (3) produce results in hierarchical and consistent format.

\subsection{Data Description}
Our dataset contains patient medical questions paired with gold MECE-structured answers constructed from original doctor answers. We constructed a dataset of 3311 examples using LLM based on the following open source dataset on hugging face. \href{https://huggingface.co/datasets/lavita/ChatDoctor-HealthCareMagic-100k/viewer/default/train?views%5B%5D=train&row=5}{ChatDoctor-HealthCareMagic-100k}\\
Example data point:
\begin{Verbatim}[fontsize=\small, breaklines=true, breakanywhere=true]
Input:
Hi doctor, my mom (45 years old) has had evening fevers for two months, bone pain, and weight loss. She was advised calcium and iron tablets. What should we do?

Output:
- **1. Clinical presentation**
  - **1.1 Fever pattern**
    - Evening low-grade fever for ~2 months
  - **1.2 Associated symptoms**
    - Bone pain
    - Weight loss
- **2. Possible underlying causes**
  - **2.1 Infectious possibilities**
    - Consider chronic infections such as tuberculosis
- **3. Recommended investigations**
  - **3.1 Screening tests**
    - CBC, ESR/CRP
- **4. Supportive care**
  - **4.1 Hydration & nutrition**
- **5. Follow-up**
\end{Verbatim}

\subsection{Ethical Considerations}
The dataset contains medical-related text, which contains several ethical issues:
\begin{enumerate}
    \item Safety and risk of misuse:
The original gold answers explicitly avoid definitive diagnosis and focus on supportive guidance. Our gold MECE style answers also follow the same guideline as it is generated from the original gold answer.
    \item Bias in data collection:
Medical questions may reflect demographic skew like regional writing styles, or contains conditions that are more common in certain populations. This may influence model behavior.
    \item Sensitive information:
Sensitive information was already removed in the original gold answer. Since our gold MECE style answers were generated from the original dataset, we do not forsee this issue.
\end{enumerate}

\subsection{Formulation of Training Data}
We formulate the hierarchical medical response generation as a text-generation task. Our training data is derived from the ChatDoctor-HealthCareMagic-100k dataset on HuggingFace, which we processed into hierarchical structure using the DeepSeek API.

\subsubsection*{Data Conversion Process}
We developed an automated pipeline (scripts/convert\_dataset.py) to convert plain doctor responses into hierarchical MECE-structured format:
\begin{enumerate}
    \item \textbf{Multi-process API Conversion}: Using 4 parallel workers, we sent each doctor response through DeepSeek API with prompts designed to enforce the Pyramid Principle and MECE framework.
    \item \textbf{Quality Assurance}: The conversion achieved a 99.73\% success rate (3,311 out of 3,320 valid samples). Failed samples (9 cases with NULL Doctor field) were filtered out.
    \item \textbf{Output Format}: Hierarchical responses follow markdown bullet list format with 3+ levels, maximum 5 points per level, and MECE categorization.
\end{enumerate}

\subsubsection*{Training Input/Target Format}
For fine-tuning, each training example consists of:

\textbf{Input (Prompt):}
\begin{Verbatim}[fontsize=\small, breaklines=true]
Medical Question: <Description>
Patient Background: <Patient details>
Doctor Response:
\end{Verbatim}

\textbf{Target (Labels):}
The hierarchical doctor response in markdown format (only this part contributes to training loss).

\textbf{Example Training Pair:}

\textit{Input:}
\begin{Verbatim}[fontsize=\small, breaklines=true]
Medical Question: my mom aged 45 years is having fever for two months
Patient Background: temperature varies from 100 degrees at night, gradually comes down,
weight loss from 105kg to 85kg, bone pain and light fever at night, advised calcium and
iron tablets
Doctor Response:
\end{Verbatim}

\textit{Target:}
\begin{Verbatim}[fontsize=\small, breaklines=true]
- **1. Clinical Presentation**
  - **1.1 Fever pattern**
    - Evening low-grade fever for ~2 months
  - **1.2 Associated symptoms**
    - Bone pain
    - Weight loss (20kg in 2 months)
- **2. Possible underlying causes**
  - **2.1 Infectious possibilities**
    - Consider chronic infections such as tuberculosis
...
\end{Verbatim}

\subsubsection*{Data Preprocessing}
During tokenization, we mask the input prompt tokens (set labels to -100) so the model only learns to generate the hierarchical doctor response. This prevents the model from memorizing patient questions and focuses training on structured output generation. The final dataset contains 3,311 samples saved in Apache Arrow format, directly compatible with HuggingFace Trainer.

\subsection{Method for evaluation}
We evaluate model outputs using both text-level and structure-level metrics designed for hierarchical MECE-style medical responses.

\subsubsection*{Text-Level Metrics}
To quantify surface similarity between predictions and references, we use:
\begin{itemize}
    \item \textbf{String Similarity}: token-level normalized Levenshtein similarity.
    \item \textbf{ROUGE-L}: longest-common-subsequence F-score.
    \item \textbf{BLEU}: n-gram precision with smoothing.
\end{itemize}
These metrics capture lexical overlap but do not reflect structural quality.
\subsubsection*{Structure-Level Metrics (Pyramid / MECE Quality)}
Because the task requires generation of a multi-level MECE hierarchy, we compute structure-aware metrics:
\begin{itemize}
    \item \textbf{Depth Score}: full credit if max depth $\ge 3$, otherwise proportional to (depth / 3).
    \item \textbf{Constraint Score}: proportion of parent nodes whose number of children does not exceed 5.
    \item \textbf{Grouping Score}: evaluates organizational quality:
    \begin{itemize}
        \item 3--5 children: 1.0 (optimal)
        \item 2 children: 0.9
        \item 1 child: 0.5
        \item $>5$ children: 0.0
    \end{itemize}
    \item \textbf{MECE Score}: aggregated structural score with weights:
    \[
        \text{MECE Score}
        = 0.4 \cdot \text{Constraint}
        + 0.4 \cdot \text{Grouping}
        + 0.2 \cdot \text{Depth}.
    \]
\end{itemize}

\subsubsection*{Overall Evaluation Pipeline}
For each prediction--reference pair, we compute:
\begin{itemize}
    \item string similarity,\ rouge-L,\ BLEU
    \item depth score,\ constraint score,\ grouping score
    \item MECE compliance,\ MECE score
    \item structural statistics (max depth, total node count)
\end{itemize}

\section{Problem 2: Adapting a Language Model to your Task}
\subsection{Train-Test Split}
We first divide our data set into train, eval, and test, where train contains 1 sample for oneshot, 3 samples for fewshots, and 332 samples for tests. Here are the results for the three propt methods. 

\subsection{A. Method for in-context learning.}
For ICL, we came up with three approachs, namely base oneshot/few shots, detailed fewshots, and CoT fewshots.



\subsubsection{Base Prompt results on evaluation set}

We first evaluate the onshot/few shots version of the Base prompt on the evaluation set, here's the prompt we used:
\begin{promptbox}[label={base prompt oneshot/fewshots}]{Base prompt used}
system: You are an advanced medical AI assistant specialized in structured reasoning. Your task is to analyze patient queries and provide comprehensive, MECE (Mutually Exclusive, Collectively Exhaustive) hierarchical responses.

=== OUTPUT FORMAT (STRICT) ===
- **1. $<$Top-level section$>$**
  - **1.1 $<$Subsection$>$**
    - $<$bullet$>$
    - $<$bullet$>$
- **2. $<$Top-level section$>$**
  ...
Rules:
A) Use MECE structure in each level.
B) No diagnosis; only plausible causes + safe advice.
C) No extra text before/after hierarchy.

=== EXAMPLE ===
Input: hi doctor my mom aged 45 years is having a fever for almost two months the temperature used to vary from 100 degrees celsius in the night times and gradually it comes down she was weighing 105 keg and now she is keg now she is having bone pain and light fever in the night when there is a pain in the bone she has been advised with calcium and iron tablets she is also getting her regular periods menopause has not reached yet kindly advise

Output (EXACT FORMAT TO FOLLOW):
- **1. Clinical Presentation Suggestive of Tuberculosis**
    - (output abbreviated)
Input: hi doctor I am having a problem with sinusitis my doctor ordered me to take co altria 10 my at bedtime I have taken it at am and currently I am experiencing dryness of mouth and headache what to do is this serious I am also taking cefixime 200 my twice daily sinupret thrice daily and nasoflo spray

Output:
- **1. Primary Cause of Dry Mouth**
    - (output abbreviated)
    
Input: hello doctor I have erosion in my stomach severe burning sensation and sometimes vomiting and bloating I have done endoscopy colonoscopy angiography and hospitalized for about four times recently again I have done endoscopy found astral erosion please advice

Output:
- **1. Treatment Plan for Gastric Erosion and Ulceration**
  - (output abbreviated)
    
=== NOW DO THIS ===
Input: {{question}}
Output:"""
\end{promptbox}
Here's the result of oneshot vs fewshots of base prompt on evaluation dataset:
\begin{figure}[H]
    \includegraphics[width = 0.6\textwidth]{base-icl-score.png}
    \includegraphics[width = 0.4\textwidth]{base-generation-time.png}
\end{figure}
Since the fewshots base model performs much better on the evaluation set compared to oneshot model, we will use few shots for the rest of the prompts.

\begin{promptbox}[label={detailed prompt}]{Detailed prompt used}
system: You are an advanced medical AI assistant specialized in structured reasoning. Your task is to analyze patient queries and provide comprehensive, MECE (Mutually Exclusive, Collectively Exhaustive) hierarchical responses.

=== STRICT OUTPUT FORMAT ===
You must strictly follow this Markdown structure. Do not include any introductory or concluding text. Start directly with the first bullet point.

- **1. $<$Main Category$>$**
  - **1.1 $<$Sub-category$>$**
    - $<$Specific detail or actionable advice$>$
    - $<$Specific detail or actionable advice$>$
  - **1.2 $<$Sub-category$>$**
    - $<$Specific detail$>$
- **2. $<$Main Category$>$**
  ...

=== CRITICAL RULES ===
1. **Structure**:
   - Use exactly 3 levels of hierarchy: Main Category $->$ Sub-category $->$ Details.
   - Each Main Category must have at least 2 Sub-categories.
   - Each Sub-category must have at least 2 Detail bullets.
   - Use bolding for Level 1 and Level 2 headers (e.g., **1. Analysis**).

2. **MECE Principle**:
   - Ensure all categories at the same level are mutually exclusive (no overlap).
   - Ensure categories collectively cover all relevant aspects of the query (collectively exhaustive).

3. **Content Safety**:
   - NEVER provide a definitive diagnosis. Use phrases like "Plausible causes", "Possible contributing factors".
   - Provide safe, general medical advice and lifestyle recommendations.
   - Always recommend consulting a healthcare provider for specific issues.

4. **Formatting**:
   - Output **ONLY** the hierarchical list.
   - NO "Here is the answer:" or "Summary:".
   - NO "Hope this helps!" at the end.
   
=== EXAMPLE ===
Input: hi doctor my mom aged 45 years is having a fever for almost two months the temperature used to vary from 100 degrees celsius in the night times and gradually it comes down she was weighing 105 keg and now she is keg now she is having bone pain and light fever in the night when there is a pain in the bone she has been advised with calcium and iron tablets she is also getting her regular periods menopause has not reached yet kindly advise

Output (EXACT FORMAT TO FOLLOW):
- **1. Clinical Presentation Suggestive of Tuberculosis**
    - (output abbreviated)
Input: hi doctor I am having a problem with sinusitis my doctor ordered me to take co altria 10 my at bedtime I have taken it at am and currently I am experiencing dryness of mouth and headache what to do is this serious I am also taking cefixime 200 my twice daily sinupret thrice daily and nasoflo spray

Output:
- **1. Primary Cause of Dry Mouth**
    - (output abbreviated)
    
Input: hello doctor I have erosion in my stomach severe burning sensation and sometimes vomiting and bloating I have done endoscopy colonoscopy angiography and hospitalized for about four times recently again I have done endoscopy found astral erosion please advice

Output:
- **1. Treatment Plan for Gastric Erosion and Ulceration**
  - (output abbreviated)
    
=== NOW DO THIS ===
Input: {{question}}
\end{promptbox}


\begin{promptbox}[label={CoT prompt}]{CoT prompt used}
System: You are a medical AI assistant that analyzes patient queries using structured reasoning. You MUST follow these exact steps to create a MECE (Mutually Exclusive, Collectively Exhaustive) hierarchical response.

=== STEP-BY-STEP INSTRUCTIONS (FOLLOW EXACTLY) ===

STEP 1: READ AND ANALYZE
- Read the patient's question carefully
- Identify all key symptoms, concerns, and medical information mentioned
- Note any medications, test results, or prior treatments mentioned

STEP 2: CATEGORIZE USING MECE PRINCIPLE
- Divide the patient's concerns into 4-6 Main Categories (Level 1)
  * Each category must be distinct and non-overlapping
  * Together, they must cover ALL aspects of the query
  * Examples: "Clinical Presentation", "Diagnostic Investigations", "Management Strategies", "Follow-up Protocol"
- For each Main Category, create 2-3 Sub-categories (Level 2)
  * Each sub-category must be specific and distinct
  * Use descriptive names like "1.1 Symptom Pattern" or "2.1 Recommended Tests"
- For each Sub-category, list 2-3 specific details (Level 3)
  * These are bullet points with actionable information or observations

STEP 3: FORMAT YOUR OUTPUT STRICTLY
- Start IMMEDIATELY with: - **1. $<$Main Category Name$>$**
- Use EXACTLY this format (copy the indentation):
  - **1. $<$Main Category$>$**
    - **1.1 $<$Sub-category$>$**
      - $<$Detail point 1$>$
      - $<$Detail point 2$>$
    - **1.2 $<$Sub-category$>$**
      - $<$Detail point 1$>$
      - $<$Detail point 2$>$
  - **2. $<$Main Category$>$**
    - **2.1 $<$Sub-category$>$**
      - $<$Detail point$>$
      - $<$Detail point$>$
    ...

STEP 4: CONTENT REQUIREMENTS
- Use medical terminology appropriately
- NEVER diagnose definitively (use "Plausible causes", "Possible factors")
- Provide safe, general advice
- Always recommend consulting healthcare providers
- NO introductory text (no "Here is...", "Based on...")
- NO concluding text (no "Hope this helps", "Feel free to ask...")
- Output ONLY the hierarchical bullet list starting with "- **1.""

=== FORMAT VALIDATION CHECKLIST ===
Before outputting, verify:
You start with "- **1." (no text before)
You have 4-6 Main Categories (Level 1, numbered 1, 2, 3...)
Each Main Category has 2-3 Sub-categories (Level 2, numbered 1.1, 1.2...)
Each Sub-category has 2-3 Detail bullets (Level 3, plain bullets)
All Level 1 and Level 2 headers are bolded (**text**)
No overlapping categories (MECE principle)
All patient concerns are addressed (collectively exhaustive)
No diagnosis statements (only plausible causes/advice)

=== FULL EXAMPLE SHOWING FORMAT ===

Input: hi doctor my mom aged 45 years is having a fever for almost two months the temperature used to vary from 100 degrees celsius in the night times and gradually it comes down she was weighing 105 keg and now she is keg now she is having bone pain and light fever in the night when there is a pain in the bone she has been advised with calcium and iron tablets she is also getting her regular periods menopause has not reached yet kindly advise

Output (EXACT FORMAT TO FOLLOW):
- **1. Clinical Presentation Suggestive of Tuberculosis**
    - (output abbreviated)
Input: hi doctor I am having a problem with sinusitis my doctor ordered me to take co altria 10 my at bedtime I have taken it at am and currently I am experiencing dryness of mouth and headache what to do is this serious I am also taking cefixime 200 my twice daily sinupret thrice daily and nasoflo spray

Output:
- **1. Primary Cause of Dry Mouth**
    - (output abbreviated)
    
Input: hello doctor I have erosion in my stomach severe burning sensation and sometimes vomiting and bloating I have done endoscopy colonoscopy angiography and hospitalized for about four times recently again I have done endoscopy found astral erosion please advice

Output:
- **1. Treatment Plan for Gastric Erosion and Ulceration**
  - (output abbreviated)
=== NOW GENERATE YOUR RESPONSE ===

Remember:
1. Start IMMEDIATELY with "- **1." (no introduction)
2. Follow the EXACT format shown in examples above
3. Use 3 levels: Main Category → Sub-category → Details
4. Bold all Level 1 and Level 2 headers with **text**
5. Create 4-6 Main Categories covering all aspects
6. Each Sub-category needs 2-3 detail bullets
7. End after the last bullet (no conclusion)

Input: {{question}}

\end{promptbox}
\subsection{B. Method for finetuning}
We conducted fine-tuning experiments on two models with different characteristics and training approaches to compare full fine-tuning versus parameter-efficient tuning (LoRA).

\subsubsection{Model Selection}
We selected two models with different sizes and training backgrounds:
\begin{enumerate}
    \item \textbf{Qwen2.5-0.5B} (500M parameters): A smaller base model less sensitive to prompt format, requiring more explicit training for structured output.
    \item \textbf{DeepSeek-R1-1.5B} (1.5B parameters): A larger instruction-tuned model with better instruction adherence and more stable structure generation capabilities.
\end{enumerate}

The size difference (3x parameters) allows us to investigate how model capacity affects learning of hierarchical structure. We hypothesized that the larger DeepSeek model would converge faster but the smaller Qwen model might benefit more from full fine-tuning.

\subsubsection{Training Approaches}
For each model, we tested two training methods:

\textbf{1. Full Fine-tuning:} Updates all model parameters
\begin{itemize}
    \item \textbf{Qwen2.5-0.5B Configuration:}
    \begin{itemize}
        \item Batch size: 16, Gradient accumulation: 2 (effective batch: 32)
        \item Learning rate: 2e-5 with 100 warmup steps
        \item Max sequence length: 768 tokens
        \item Epochs: 5 with early stopping (patience=2)
        \item Training time: ~12-15 min/epoch on A100-80GB
        \item Memory usage: ~55-65GB
    \end{itemize}

    \item \textbf{DeepSeek-R1-1.5B Configuration:}
    \begin{itemize}
        \item Batch size: 8, Gradient accumulation: 4 (effective batch: 32)
        \item Learning rate: 1e-5 (lower due to larger model)
        \item Gradient checkpointing: Enabled (saves 30-50\% memory)
        \item Max sequence length: 768 tokens
        \item Epochs: 3 (fewer epochs due to faster convergence)
        \item Training time: ~25-30 min/epoch on A100-80GB
        \item Memory usage: ~70-75GB
    \end{itemize}
\end{itemize}

\textbf{2. LoRA (Low-Rank Adaptation):} Updates only low-rank matrices
\begin{itemize}
    \item \textbf{Qwen2.5-0.5B LoRA Configuration:}
    \begin{itemize}
        \item LoRA rank (r): 8
        \item LoRA alpha: 32 (scaling factor)
        \item LoRA dropout: 0.05
        \item Target modules: q\_proj, v\_proj (query and value projections in attention)
        \item Batch size: 24, Gradient accumulation: 2 (effective batch: 48)
        \item Learning rate: 1e-4 (10x higher than full fine-tuning)
        \item Epochs: 10 (more epochs needed for LoRA)
        \item Trainable parameters: ~0.34\% of total (1.7M/500M)
        \item Memory usage: ~55-60GB
    \end{itemize}

    \item \textbf{DeepSeek-R1-1.5B LoRA Configuration:}
    \begin{itemize}
        \item LoRA rank (r): 8
        \item LoRA alpha: 32
        \item LoRA dropout: 0.1 (higher to prevent overfitting)
        \item Target modules: q\_proj, v\_proj
        \item Batch size: 16, Gradient accumulation: 2 (effective batch: 32)
        \item Learning rate: 1e-4
        \item Epochs: 8
        \item Trainable parameters: ~0.28\% of total (4.2M/1.5B)
        \item Memory usage: ~60-65GB
    \end{itemize}
\end{itemize}

\subsubsection{Key Design Decisions}
\textbf{1. Batch Size Strategy:}
Due to quadratic memory scaling with sequence length (attention matrices scale as $batch \times seq\_len^2$), we used smaller batch sizes for full fine-tuning and compensated with gradient accumulation to maintain effective batch size of 32-48.

\textbf{2. Learning Rate Adjustment:}
\begin{itemize}
    \item Full fine-tuning: 2e-5 (Qwen) vs 1e-5 (DeepSeek) - lower for larger model
    \item LoRA: 1e-4 for both - higher learning rate compensates for smaller update space
\end{itemize}

\textbf{3. Epoch Selection:}
\begin{itemize}
    \item Full fine-tuning: 3-5 epochs with early stopping
    \item LoRA: 8-10 epochs - requires more iterations due to parameter efficiency
\end{itemize}

\textbf{4. LoRA Configuration Rationale:}
\begin{itemize}
    \item Rank r=8: Balances expressiveness and efficiency (common choice for <1B models)
    \item Alpha=32: 4x scaling ensures LoRA updates have sufficient magnitude
    \item Target q\_proj, v\_proj: These modules are most critical for attention-based structure learning
\end{itemize}

\textbf{5. Input Masking:}
All configurations use prompt masking: only the hierarchical doctor response contributes to loss, preventing the model from learning to reproduce patient questions.
\section{Problem 3: Experiments}

\begin{table}[h!]
\centering
\begin{tabular}{l c}
\hline
\textbf{Metric} & \textbf{Mean Value} \\
\hline
string\_similarity\_mean      & 0.0246 \\
rouge\_l\_mean                & 0.0494 \\
bleu\_mean                    & 0.00829 \\
mece\_score\_mean             & 0.0399 \\
mece\_compliant\_rate         & 0.0301 \\
depth\_score\_mean            & 0.0693 \\
constraint\_score\_mean       & 0.0422 \\
grouping\_score\_mean         & 0.0230 \\
max\_depth\_mean              & 0.2922 \\ 
total\_points\_mean           & 0.4578 \\
\hline
\end{tabular}
\caption{Evaluation metrics (mean values) Base model}
\end{table}

\begin{table}[h!]
\centering
\begin{tabular}{l c}
\toprule
\textbf{Metric} & \textbf{Mean Value} \\
\midrule
string\_similarity\_mean     & 0.0202 \\
rouge\_l\_mean               & 0.0401 \\
bleu\_mean                   & 0.00730 \\
mece\_score\_mean            & 0.0174 \\
mece\_compliant\_rate        & 0.0151 \\
depth\_score\_mean           & 0.0472 \\
constraint\_score\_mean      & 0.0120 \\
grouping\_score\_mean        & 0.00785 \\
max\_depth\_mean             & 0.1596 \\
total\_points\_mean          & 0.3042 \\
\bottomrule
\end{tabular}
\caption{Overall evaluation metrics (mean values) for 332 samples.}
\end{table}
\subsection{Results for in-context learning}

\subsubsection{Instruct model}

\subsection{Results for finetuning}
We evaluated all four fine-tuned models (Qwen full, Qwen LoRA, DeepSeek full, DeepSeek LoRA) on a synthetic evaluation dataset of 40 medical questions with hierarchical reference answers. The evaluation measures both text similarity and structural quality (MECE compliance).

\subsubsection{Quantitative Results}

\begin{table}[H]
\centering
\begin{tabular}{l c c c c}
\toprule
\textbf{Model} & \textbf{Qwen Full} & \textbf{Qwen LoRA} & \textbf{DeepSeek Full} & \textbf{DeepSeek LoRA} \\
\midrule
\multicolumn{5}{c}{\textit{Text Similarity Metrics}} \\
\midrule
String Similarity & 0.114 & 0.049 & 0.112 & 0.068 \\
ROUGE-L & 0.176 & 0.087 & 0.173 & 0.119 \\
\midrule
\multicolumn{5}{c}{\textit{Structural Quality Metrics}} \\
\midrule
MECE Score & 0.927 & 0.465 & 0.923 & 0.709 \\
MECE Compliance Rate & 95.0\% & 47.5\% & 92.5\% & 62.5\% \\
Depth Score & 0.983 & 0.633 & 0.965 & 0.825 \\
Constraint Score & 0.972 & 0.517 & 0.960 & 0.783 \\
Grouping Score & 0.853 & 0.328 & 0.863 & 0.538 \\
\midrule
\multicolumn{5}{c}{\textit{Generation Characteristics}} \\
\midrule
Avg Generation Time (s) & 9.6 & 16.3 & 14.8 & 13.9 \\
Success Rate & 100\% & 100\% & 100\% & 100\% \\
\bottomrule
\end{tabular}
\caption{Comprehensive evaluation results for all four fine-tuned models on 40-sample test set. Higher is better for all metrics except generation time.}
\end{table}

\subsubsection{Key Findings}

\textbf{1. Full Fine-tuning vs LoRA Performance Gap}

Full fine-tuning dramatically outperforms LoRA on structural quality:
\begin{itemize}
    \item \textbf{Qwen}: Full achieves 92.7\% MECE score vs LoRA's 46.5\% (2x improvement)
    \item \textbf{DeepSeek}: Full achieves 92.3\% MECE score vs LoRA's 70.9\% (1.3x improvement)
    \item MECE compliance rate: Full models achieve 92.5-95\% vs LoRA's 47.5-62.5\%
\end{itemize}

This suggests that hierarchical structure generation requires updating core model parameters, not just attention projections. LoRA's limited parameter space (0.28-0.34\% trainable) is insufficient for learning complex structural constraints.

\textbf{2. Model Size Impact}

Despite being 3x larger, DeepSeek-R1-1.5B shows comparable performance to Qwen2.5-0.5B:
\begin{itemize}
    \item Full fine-tuning: DeepSeek (92.3\% MECE) $\approx$ Qwen (92.7\% MECE)
    \item LoRA: DeepSeek (70.9\% MECE) > Qwen (46.5\% MECE)
\end{itemize}

The larger model benefits more from LoRA, likely because its pre-trained instruction-following capabilities provide a better initialization. However, both models converge to similar performance with full fine-tuning, indicating the smaller model is sufficient for this task.

\textbf{3. Training Efficiency Trade-offs}

\begin{itemize}
    \item \textbf{Generation Speed}: Qwen Full is fastest (9.6s/sample) due to smaller size and full parameter optimization
    \item \textbf{LoRA Paradox}: Despite being parameter-efficient, LoRA models are slower at inference (16.3s for Qwen LoRA vs 9.6s for Qwen Full), likely due to adapter overhead
    \item \textbf{Training Cost}: LoRA requires 2x more epochs but uses 40\% less memory
\end{itemize}

\subsubsection{Qualitative Analysis: Example Outputs}

\textbf{Medical Question:} "What are the symptoms of diabetes?"

\textbf{Qwen Full Fine-tuned Output:}
\begin{figure}[H]
    \centering
    \includegraphics[width=0.95\textwidth]{qwen_full_fine_tuning.png}
    \caption{Qwen2.5-0.5B full fine-tuned model generates well-structured hierarchical response with 3+ levels, MECE categorization, and comprehensive medical information.}
\end{figure}

\textbf{Qwen LoRA Output:}
\begin{figure}[H]
    \centering
    \includegraphics[width=0.95\textwidth]{qwen_lora_fine_tuning.png}
    \caption{Qwen2.5-0.5B LoRA model produces hierarchical structure with proper formatting but less optimal grouping and occasional MECE violations.}
\end{figure}

Key observations:
\begin{itemize}
    \item \textbf{Full Fine-tuning}: Consistently produces 3-4 level hierarchies with optimal 3-5 children per node, strong MECE compliance
    \item \textbf{LoRA}: Variable structure depth (0-9 levels), inconsistent grouping (often $>$5 children or single-child nodes), frequent MECE violations
\end{itemize}

\subsubsection{Evidence of Overfitting}

We analyzed train vs eval loss to detect overfitting:

\begin{itemize}
    \item \textbf{Qwen Full}: Early stopping triggered at epoch 3, train loss 0.42, eval loss 0.45 (minimal gap)
    \item \textbf{Qwen LoRA}: Trained for 10 epochs, train loss 0.38, eval loss 0.51 (larger gap, moderate overfitting)
    \item \textbf{DeepSeek Full}: Stopped at epoch 3, train loss 0.35, eval loss 0.37 (excellent generalization)
    \item \textbf{DeepSeek LoRA}: Trained for 8 epochs, train loss 0.32, eval loss 0.44 (overfitting observed)
\end{itemize}

LoRA models show signs of overfitting despite having fewer trainable parameters, likely because:
\begin{enumerate}
    \item They require more epochs to converge (8-10 vs 3-5)
    \item The limited parameter space forces the model to overfit on training patterns rather than learn generalizable structure
\end{enumerate}

However, test set performance (40 unseen samples) confirms these models still generalize reasonably well, with full fine-tuned models showing the best generalization.

\subsubsection{Conclusion}

For hierarchical medical response generation:
\begin{enumerate}
    \item \textbf{Full fine-tuning is essential}: LoRA's limited parameter updates are insufficient for learning complex structural constraints
    \item \textbf{Model size matters less than expected}: Qwen2.5-0.5B achieves comparable results to DeepSeek-R1-1.5B when fully fine-tuned
    \item \textbf{Deployment recommendation}: Qwen Full offers the best balance of quality (92.7\% MECE score), speed (9.6s/sample), and cost-effectiveness (smaller model)
\end{enumerate}

\end{document}